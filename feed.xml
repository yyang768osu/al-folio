<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://yyang768osu.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://yyang768osu.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2022-05-25T02:06:21+00:00</updated><id>https://yyang768osu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Quantization for Neural Networks</title><link href="https://yyang768osu.github.io/blog/2022/neural-network-quantization/" rel="alternate" type="text/html" title="Quantization for Neural Networks" /><published>2022-03-05T00:00:00+00:00</published><updated>2022-03-05T00:00:00+00:00</updated><id>https://yyang768osu.github.io/blog/2022/neural-network-quantization</id><content type="html" xml:base="https://yyang768osu.github.io/blog/2022/neural-network-quantization/"><![CDATA[<blockquote>
  <p>This post is a reading note of the paper <a href="https://arxiv.org/abs/2106.08295">A White Paper on Neural Network Quantization</a> <a href="#Nagel_et_al_2021">[Nagel et al. 2021]</a> with additional illustrations and explanations that help my understanding (hopefully can help yours too). The same material can be found in the paper and the references therein. You are highly encouraged to read the paper first and the references if you want to go into depth for certain topics.</p>
</blockquote>

<p>Quantization of neural network model enables fast inference using efficient fixed point operations in typical AI hardware accelerators. Compared with floating point inference, it leads to less storage space, smaller memory footprint, lower power consumption, and faster inference speed, all of which are essential for practical edge deployment. It is thus a critical step in the model efficient pipeline. In this post, based off <a href="#Nagel_et_al_2021">[Nagel et al. 2021]</a>, we provide a detailed guide to network quantization.</p>

<hr />

<p>We start off with the basics in the first section by first going through the conversion of floating point and fixed point representations and introduce necessary notations. Followed by showing how fixed point arithmetic works in a typical hardware accelerator and the way to simulate it using floating point operation. And lastly define terminology of Post-Training-Quantization (PTQ) and Quantization-Aware-Training (QAT).</p>

<p>The second section covers four techniques to improve quantization performance, which are mostly designed targeting PTQ, with some also serving as a necessary step to get good initialization for QAT.</p>

<p>In the last part, we go over the recommended pipeline and practices of PTQ and QAT as suggested by <a href="https://arxiv.org/abs/2106.08295">the white paper</a> <a href="#Nagel_et_al_2021">[Nagel et al. 2021]</a> and discuss how special layers other than Conv/FC can be handled. Here’s an outline.</p>

<ul id="markdown-toc">
  <li><a href="#quantization-basics" id="markdown-toc-quantization-basics">Quantization basics</a>    <ul>
      <li><a href="#conversion-of-floating-point-and-fixed-point-representation" id="markdown-toc-conversion-of-floating-point-and-fixed-point-representation">Conversion of floating point and fixed point representation</a></li>
      <li><a href="#fixed-point-arithmetic" id="markdown-toc-fixed-point-arithmetic">Fixed point arithmetic</a></li>
      <li><a href="#per-tensor-vs-per-channel-quantization" id="markdown-toc-per-tensor-vs-per-channel-quantization">Per-tensor vs per-channel quantization</a></li>
      <li><a href="#quantization-simulation-and-gradient-computation" id="markdown-toc-quantization-simulation-and-gradient-computation">Quantization simulation and gradient computation</a></li>
      <li><a href="#post-training-quantization-ptq-vs-quantization-aware-training-qat" id="markdown-toc-post-training-quantization-ptq-vs-quantization-aware-training-qat">Post-Training-Quantization (PTQ) vs Quantization-Aware-Training (QAT)</a></li>
    </ul>
  </li>
  <li><a href="#quantization-techniques" id="markdown-toc-quantization-techniques">Quantization techniques</a>    <ul>
      <li><a href="#range-estimation-methods" id="markdown-toc-range-estimation-methods">Range Estimation Methods</a></li>
      <li><a href="#cross-layer-equalization" id="markdown-toc-cross-layer-equalization">Cross Layer Equalization</a></li>
      <li><a href="#bias-correction" id="markdown-toc-bias-correction">Bias Correction</a></li>
      <li><a href="#adaptive-rounding" id="markdown-toc-adaptive-rounding">Adaptive Rounding</a></li>
    </ul>
  </li>
  <li><a href="#ptq-and-qat-best-practices" id="markdown-toc-ptq-and-qat-best-practices">PTQ and QAT best practices</a>    <ul>
      <li><a href="#ptq-pipeline-and-debugging-strategy" id="markdown-toc-ptq-pipeline-and-debugging-strategy">PTQ pipeline and debugging strategy</a></li>
      <li><a href="#qat-pipeline" id="markdown-toc-qat-pipeline">QAT pipeline</a></li>
    </ul>
  </li>
  <li><a href="#treatment-of-special-layers" id="markdown-toc-treatment-of-special-layers">Treatment of special layers</a></li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

<hr />
<h1 id="quantization-basics">Quantization basics</h1>

<hr />
<h3 id="conversion-of-floating-point-and-fixed-point-representation">Conversion of floating point and fixed point representation</h3>
<hr />

<p>Fixed point and floating point are different ways of representing numbers in computing devices. Floating point representation is designed to capture fine precisions (by dividing the bit field into mantissa and exponent) with high bit-width (typically 32 bits or 64 bits), whereas fixed point numbers lives on a fixed width grid that are typically much coarser (typically 4, 8, 16 bits). Due to its simplicity, the circuitry for fixed point arithmetic can be much cheaper, simpler, and more efficient than its floating point counterpart. The term <em>network quantization</em> refers to the process of converting neural network models with floating point weight and operations into one with fixed point weight and operations for better efficiency.</p>

<p>The figure below describes how the conversion from floating point number to fixed point integer can be done. Let us label the start and the end of the fixed point integer number as \(n\) and \(p\). For signed integer with bit-width of \(b\), we have \([n, p]=[-2^{b-1}, 2^{b-1}-1]\); for unsigned integer \([n, p]=[0, 2^{b}-1]\). In this simple example \(b=4\), which gives us 16 points on the grid.</p>

<p>Assume that we have a conversion scheme that maps a floating point \(0\) to an integer \(z\), and \(s\) to \(z+1\). The mapping of floating point number \(x\) to its fixed point representation \(x_\text{int}\) can be described as</p>

\[\begin{align}
x_\text{int} = \texttt{clamp}(\lfloor x/s \rceil + z; n, p),\label{eq:xint_x}
\end{align}\]

<p>where \(\lfloor\cdot\rceil\) denotes rounding-to-nearest operation and \(\texttt{clamp}(\cdot: n, p)\) clips its input to within \([n, p]\). \(s\) and \(z\) are often referred to as <em>scale</em> and <em>offset</em>.</p>

<div class="row mt-3">
    <div class="col-sm-1 mt-3 mt-md-0">
    </div>
    <div class="col-sm-10 mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/quantization/quantization-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/quantization/quantization-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/quantization/quantization-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/quantization/quantization.png" />

  </picture>

</figure>

    </div>
    <div class="col-sm-1 mt-3 mt-md-0">
    </div>
</div>

<p>The integer \(x_\text{int}\), when mapped back to the floating point axis, corresponds to</p>

\[\begin{align}
\hat{x} = s\left(x_\text{int} - z\right).\label{eq:xhat_xint}
\end{align}\]

<p>By combining the two equations, we can express the quantized floating point value \(\hat{x}\) as</p>

\[\begin{align}
\hat{x} =&amp; s \left(\texttt{clamp}\left(\left\lfloor x/s\right\rceil +z; n, p\right)-z\right) \notag\\
        =&amp; \texttt{clamp}\left(s\left\lfloor x/s \right\rceil; s(n-z), s(p-z)\right) \notag\\
        =&amp; \texttt{clamp}{\Big(}\underbrace{s\left\lfloor x/s \right\rceil}_{\substack{\text{rounding}\\\text{error}}}; \underbrace{q_\text{min}, q_\text{max}}_{\substack{\text{clipping}\\\text{error}}}{\Big)}. \label{eq:x_hat}
\end{align}\]

<p>Here we denote \(q_\text{min}\triangleq s(n-z)\) and \(q_\text{max}\triangleq s(p-z)\) as the floating point values corresponding to two limits on the fixed point grid. The quantization error of \(x-\hat{x}\) comes from two competing factors: clipping and rounding. Increasing \(s\) reduces the clipping error at the cost of increased rounding error. Decreasing \(s\) reduces rounding error with higher chance for clipping. This is referred to as <em>range and precision tradeoff</em>.</p>

<hr />

<p>To avoid getting “lost in notation” for the many that we introduced (\(n\), \(p\), \(b\), \(z\), \(s\), \(q_\text{min}\), \(q_\text{max}\), \(x\), \(\hat{x}\), \(x_\text{int}\)), it is important to keep the following in mind:</p>
<ul>
  <li>\(n\), \(p\), and \(b\) are determined by the integer type that is available to us. They describe the hardware constraints and are not something we can control (<em>nor is there a need to control them!</em>)
    <ul>
      <li>For 8-bit signed integer we have \(n=-128\) and \(p=127\)</li>
      <li>For 8-bit unsigned integer we have \(n=0\) and \(p=255\)</li>
      <li>For bit-width of \(b\), we have \(p-n+1 = 2^b\)</li>
    </ul>
  </li>
  <li>Either \((s, z)\) or \((q_\text{min}, q_\text{max})\) uniquely describes the quantization scheme. Together they are redundant since one can derive the other. When we talk about <strong>quantization parameters</strong>, we refer to either \((s, z)\), or equivalently \((q_\text{min}, q_\text{max})\), since there are only <strong>two degrees of freedom</strong> in  \((s, z, q_\text{min}, q_\text{max})\).
    <ul>
      <li>Derive \((q_\text{min}, q_\text{max})\) from \((s, z)\):
        <ul>
          <li>\(q_\text{min} = s(n-z)\).</li>
          <li>\(q_\text{max} = s(p-z)\).</li>
        </ul>
      </li>
      <li>Derive \((s, z)\) from \((q_\text{min}, q_\text{max})\):
        <ul>
          <li>\(s = (q_\text{max} - q_\text{min}) / 2^b\).</li>
          <li>\(z = n - q_\text{min}/s = p - q_\text{max}/s\).</li>
        </ul>
      </li>
      <li>\(z=0\) is referred to as <strong>symmetric quantization</strong>
        <ul>
          <li>Note that <strong>symmetric quantization</strong> does not imply that we have equal number of grid points on either side of integer \(0\), but rather just the fact that floating point \(0\) is mapped to integer \(0\) in the fixed point representation</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Among the different variables, \(z\) and \(x_\text{int}\) are fixed point number whereas \(x\), \(\hat{x}\), \(s\) are floating point numbers
    <ul>
      <li>We want to carry out all operations in the fixed point domain, so ideally only offset \(z\) and \(x_\text{int}\) should be involved in hardware multiplication and summation.</li>
    </ul>
  </li>
  <li>The fact that floating point \(0\) maps to an integer \(z\) ensures that there is no quantization error in representing floating point \(0\)
    <ul>
      <li>This guarantees that zero-padding and ReLU do not introduce quantization error</li>
    </ul>
  </li>
</ul>

<hr />

<p>An important take-away here is that there are two equivalent definitions of quantization parameters:</p>

<blockquote>
  <ul>
    <li><strong>Quantization parameters</strong> are either \((s, z)\) or \((q_\text{min}, q_\text{max})\)
      <ul>
        <li>In the context of QAT, we talk about \((s, z)\) more as they are directly trainable</li>
        <li>In the context of PTQ, \((q_\text{min}, q_\text{max})\) is used more often</li>
        <li><strong>Range estimation</strong> refers to the estimation of \((q_\text{min}, q_\text{max})\)</li>
      </ul>
    </li>
  </ul>
</blockquote>

<hr />
<h3 id="fixed-point-arithmetic">Fixed point arithmetic</h3>
<hr />

<p>Now that we have covered  how to map number from floating point to fixed point format, next we will show how we can use fixed point arithmetic to approximate floating point operations.</p>

<p>Let’s first take a look at an easy case of scalar multiplication \(y = wx\), and denote the quantization parameters of \(w, x, y\) as \((s_w, z_w), (s_x, z_x), (s_y, z_y)\). From Equation \eqref{eq:xhat_xint}, the quantized version of \(w\) and \(x\) can be expressed as</p>

\[\begin{align*}
\hat{w} =&amp; s_w(w_\text{int} - z_w)\\
\hat{x} =&amp; s_x(x_\text{int} - z_x)\\
\end{align*}\]

<p>The product of \(wx\) can then be approximated by \(\hat{w}\hat{x}\). In what follows we will highlight fixed point numbers as blue to make it clear which of the operations are carried out in fixed point domain.</p>

\[\begin{align*}
wx \approx&amp; \hat{w}\hat{x} \\
  =&amp; s_w({\color{blue}w_\text{int}} - {\color{blue}z_w})*s_x({\color{blue}x_\text{int}} - {\color{blue}z_x})\\
  =&amp; s_w s_x \left[ ({\color{blue}w_\text{int}}-{\color{blue}z_w})({\color{blue}x_\text{int}}-{\color{blue}z_x})\right] \\
  =&amp; \underbrace{s_w s_x}_{\substack{\text{scale of}\\\text{product}}} 
  {\color{blue}\big[} 
  {\color{blue}w_\text{int} x_\text{int}} 
  - \underbrace{\color{blue}z_wx_\text{int}}_{\substack{\text{input }x\\\text{dependent}}} 
  - \underbrace{\color{blue}w_\text{int}z_x + z_wz_x}_{\substack{\text{can be}\\\text{pre-computed}}} 
  {\color{blue}\big]}
\end{align*}\]

<p>It should be clear that all terms inside the square bracket above can be carried out in fixed point operations. Since integer multiplication will inflate bit-width (the product of two 8-bit numbers will take 16-bit), and also we need to accumulate different terms, the result inside the bracket is typically stored in high bit-width accumulator (e.g., 32-bit accumulator).</p>

<p>We are not done yet – we still need to fit \(\hat{w}\hat{x}\) to the quantization grid chosen for \(y\). In other words, we need to map it to the fixed point format indicated by \((s_y, z_y)\) using the Equation \eqref{eq:xint_x}, a step referred to as <strong>requantization</strong>:</p>

\[\begin{align*}
y_\text{int} =&amp; \texttt{clamp}\left(\left\lfloor \frac{\hat{w}\hat{x}}{s_y} \right\rceil + z_y; n, p\right)\\
             =&amp; \texttt{clamp}\left(\left\lfloor \frac{s_w s_x}{s_y}\times{\color{blue}\text{Accumulator}} \right\rceil + z_y; n, p\right).
\end{align*}\]

<p>In the figure below, we illustrate an example when \(z_w\) \(z_x\) and \(z_y\) are all \(0\).</p>

<div class="row mt-3">
    <div class="col-sm-2 mt-3 mt-md-0">
    </div>
    <div class="col-sm-8 mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/quantization/requantization-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/quantization/requantization-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/quantization/requantization-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/quantization/requantization.png" />

  </picture>

</figure>

    </div>
    <div class="col-sm-2 mt-3 mt-md-0">
    </div>
</div>

<p>The problem here is that \(\frac{s_w s_x}{s_y}\) is a floating point number, and we need to multiply it with the fixed point value in the high bit-width accumulator. This can be carried out in hardware if we approximate \(\frac{s_w s_x}{s_y}\) with the form of \({\color{blue}a}2^{n}\) where \({\color{blue}a}\) is a fixed point number and \(n\) is an integer, in which case requantization can be efficiently done by the multiplication of two fixed point numbers (value in the high bit-width accumulator, and \(a\)) followed by a bit-shift. How this operation is carried out can be hardware specific. The procedure of requantization are also described in Equation (5) and (6) in <a href="#Jacob_et_al_2018">[Jacob et al. 2018]</a>.</p>

<hr />

<p>Next we generalize the simple case of scalar multiplication to matrix-vector product with bias \(y = W x + b\). For the weight matrix \(W\), the \(i^\text{th}\) row is responsible for computing the \(i^\text{th}\) output element, which is referred to as <strong>output channel</strong>, expressed as below</p>

\[\begin{align}
y_i =&amp; \sum_{j}W_{ij}x_j + b_i\notag\\
\approx&amp; \sum_{j}\hat{W}_{ij}\hat{x}_j + \hat{b}_i \notag\\
=&amp; \sum_{j} s_i^w \left(W_{ij}^\text{int} - z_i^w\right) s^x \left(x_j^\text{int}- z^x\right) + s_i^ws^x b_i^\text{int}\notag\\
=&amp; s_i^ws^x {\bigg[}
  \sum_{j}{\big(}
    W_{ij}^\text{int}x_j^\text{int} - z_i^wx_j^\text{int} - W_{ij}^\text{int}z^x + z_i^wz^x
  {\big)}
  +
  b_i^\text{int}
{\bigg]}\notag\\
=&amp; s_i^ws^x {\color{blue}\bigg[}
    \underbrace{\color{blue}\sum_{j}{\big(}
    W_{ij}^\text{int}x_j^\text{int}{\big)}}_{\substack{\text{Multiply-Accumulate}\\\text{(MAC)}}} - 
    \underbrace{\color{blue}\sum_{j}{\big(}z_i^wx_j^\text{int}{\big)}}_{\substack{\text{No-op if}\\\text{ weight offset is }0}} + 
    \underbrace{\color{blue}\sum_{j}{\big(}- W_{ij}^\text{int}z^x + z_i^wz^x
  {\big)}
  +
  b_i^\text{int}}_{
    \substack{\text{Can be pre-computed and }\\\text{pre-loaded to the accumulator}}}
{\color{blue}\bigg]} \label{eq:y_i}
\end{align}\]

<p>A few remarks are due:</p>
<ul>
  <li>Treatment of bias quantization:
    <ul>
      <li>It should be clear that in order for the bias to be easily added to the accumulator, we should fix its scale to be \(s_i^b = s_i^ws^x\). In other words, the scale of bias is a not a free variable that we can control.</li>
      <li>Since \(b_\text{int}\) can be directly added to the high bit-width accumulator directly, we normally don’t need to be concerned regarding clipping error of bias quantization.</li>
    </ul>
  </li>
  <li>For the four terms inside the square bracket
    <ul>
      <li>The last two are not data dependent and can be pre-computed, merged with bias, and pre-loaded to the accumulator together, with no additional inference cost.</li>
      <li>The second one, however, is data dependent, but vanishes if weight offset \(z_i^w\) is zero. To save compute, a common approach is to keep weight quantization to be symmetric (i.e., \(z_i^w=0\)) so that this terms goes away.</li>
    </ul>
  </li>
  <li>The output \(y\) typical goes through nonlinear activation functions
    <ul>
      <li>Some activation functions such as ReLU can be directly applied in the accumulator before requantization is applied.</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>A common design choice is to have <strong>symmetric weight quantization</strong> (\(z^w=0\)) and <strong>asymmetric activation quantization</strong> to avoid the data dependent term.</p>
</blockquote>

<hr />
<h3 id="per-tensor-vs-per-channel-quantization">Per-tensor vs per-channel quantization</h3>
<hr />

<p>Equation \eqref{eq:y_i} describes the operations needed to obtain the result of the \(i^\text{th}\) output channel. One assumption made is that weights for the same output channel (\(W_{i\cdot}=W_{i,1}, W_{i,2}, \ldots\) corresponds to the \(i^\text{th}\) output channel) share the same quantization parameters, denoted as \(s_i^w\) and \(z_i^w\), and that all the elements in the input tensor \(x=[x_1, x_2, \ldots]\) shares the same quantization parameters denoted as \(s^x\) and \(z^x\). In this part we review different choices of quantization granularity.</p>

<p>One common setting is to have a single set of quantization parameters for the entire weight tensor, which correspond to figure (a) illustrated below. This is referred to as <strong>per tensor quantization</strong>. In this case, all the output channels of the weights share the same scaling factor \(s^ws^x\), and thus the requantization operation is uniform across all the output channels.</p>

<p>One problem with this setting is that in practice, weights from different output channels could have a wide difference in their dynamic ranges. E.g., weights in a first channel may range from \(-500.0\) to \(500.0\) while the second from \(-1.0\) to \(1.0\). If we pick quantization parameters to \([-500, 500]\) in 8-bit, the second channel will be completely quantized away (i.e., quantized to \(0\), aka underflow), losing all its information. On the other hand, if we fit the range to \([-1, 1]\), the first channel will suffer from huge clipping error. In one of the later sections a technique called <strong>cross layer equalization</strong> is introduced to alleviate this issue by trying to equalize range for different channels.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/quantization/quantization_granularity-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/quantization/quantization_granularity-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/quantization/quantization_granularity-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/quantization/quantization_granularity.png" />

  </picture>

</figure>

    </div>
</div>

<p>A more flexible setting is to allow different channels to be quantized differently <a href="#Krishnamoorthi_2018">[Krishnamoorthi 2018]</a>, referred to as <strong>per-channel quantization of weight</strong>. This is the setting captured by figure (b) above, and is the case described in Equation \eqref{eq:y_i} where \(s_i^w\) can be different for different output channels with index \(i\). The hardware implication for this setting is that requantization is no longer uniform across different output channels. This scheme would save us from the varying dynamic range issue mentioned before, but not every hardware platform support this setting.</p>

<p>Both (a) and (b) assume that the activation is quantized in a per-tensor fashion. The alternative option of <strong>per-channel quantization of activation</strong>, as illustrated in Figure (c), is extremely hard to handle in hardware. In this setting, the accumulator needs to rescaled for each input dimension. As is evident from Equation \eqref{eq:y_i}, having a per-channel quantization of activation implies that the scaling factor for activation should be denoted as \(s_{\color{red}j}^x\), and as a result we can no longer factor \(s_i^{w}s^x_j\) outside of the summation across input dimension \(j\), meaning that each multiply-and-accumulation would require a rescaling operation. Due to its hardware difficulty, most fixed-point accelerators do not support this setting.</p>

<blockquote>
  <ul>
    <li><strong>(a) per-tensor quantization</strong>
      <ul>
        <li>base setting supported by all fixed point accelerators</li>
        <li>can suffer from large quantization error due to varying dynamic ranges across different output channels of the weight</li>
      </ul>
    </li>
    <li><strong>(b) per-channel quantization of weights</strong>
      <ul>
        <li>also simply referred to as <strong>per-channel quantization</strong></li>
        <li>it becomes a general trend, although not all hardware support it</li>
      </ul>
    </li>
    <li><strong>(c) per-channel quantization of activation</strong>
      <ul>
        <li>as of now this is not generally considered as a viable solution due to the difficulty in hardware implementation</li>
      </ul>
    </li>
  </ul>
</blockquote>

<hr />
<h3 id="quantization-simulation-and-gradient-computation">Quantization simulation and gradient computation</h3>
<hr />

<p>Fixed point operation can be simulated with floating-point training if we map the weight \(W\) and activation \(x\) values to its quantized version \(\widehat{W}\) and \(\hat{x}\). The mapping is described in Equation \eqref{eq:x_hat} and re-stated below.</p>

\[\begin{align}
\hat{x} =&amp; \texttt{clamp}{\bigg(}\underbrace{s\left\lfloor x/s \right\rceil}_{\substack{\text{rounding}\\\text{error}}}; \underbrace{\overbrace{s(n-z)}^{q_\text{min}}, \overbrace{s(p-z)}^{q_\text{max}}}_{\text{clipping error}}{\bigg)}
= \left\{
  \begin{array}{ll}
  s\left\lfloor x/s \right\rceil &amp; \text{if } q_\text{min} \leq x \leq q_\text{max} \\
  s(n-z) &amp; \text{if } x &lt; q_\text{min} \\
  s(p-z) &amp; \text{if } x &gt; q_\text{max}
  \end{array}
  \right.\label{eq:x_hat_re}
\end{align}\]

<p>The schematic below illustrate how simulated quantization described by this equation above can be added to a typical network layer.</p>

<div class="row mt-3">
    <div class="col-sm-2 mt-3 mt-md-0">
    </div>
    <div class="col-sm-8 mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/quantization/quantization_sim-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/quantization/quantization_sim-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/quantization/quantization_sim-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/quantization/quantization_sim.png" />

  </picture>

</figure>

    </div>
    <div class="col-sm-2 mt-3 mt-md-0">
    </div>
</div>
<div class="caption">
    Quantization simulation
</div>

<p>From Equation \eqref{eq:x_hat_re} we can derive the gradient of the quantized value with respect to the input as well as the quantization parameters (\(s\) and \(z\)) as the following. In order back-propagate through rounding \(\lfloor\cdot\rceil\), the straight-through estimator \(\partial \lfloor x \rceil/\partial x \approx 1\) is used (denoted as \(\overset{\text{st}}{\approx}\) below).</p>

\[\begin{align}
\frac{\partial \hat{x}}{\partial x} =&amp; \left\{
  \begin{array}{ll}
  s \frac{\partial}{\partial x}\left\lfloor x/s\right\rceil \overset{\text{st}}{\approx} 1 &amp;\text{if } q_\text{min} \leq x \leq q_\text{max} \\
  0 &amp; \text{if } x &lt; q_\text{min} \\
  0 &amp; \text{if } x &gt; q_\text{max}
  \end{array}
  \right.\label{eq:d_hatx_dx}\\

\frac{\partial \hat{x}}{\partial s} =&amp; \left\{
  \begin{array}{ll}
  \left\lfloor x/s \right\rceil + s \frac{\partial}{\partial s} \left\lfloor x/s \right\rceil \overset{\text{st}}{\approx}  \left\lfloor x/s \right\rceil - x/s  &amp;\text{if } q_\text{min} \leq x \leq q_\text{max} \\
  n - z  &amp; \text{if } x &lt; q_\text{min} \\
  p - z &amp; \text{if } x &gt; q_\text{max}
  \end{array}
  \right. \label{eq:d_hatx_ds}\\
  
\frac{\partial \hat{x}}{\partial z} =&amp; \left\{
  \begin{array}{ll}
  0 &amp;\text{if } q_\text{min} \leq x \leq q_\text{max} \\
  -s &amp; \text{if } x &lt; q_\text{min} \\
  -s &amp; \text{if } x &gt; q_\text{max}
  \end{array}
  \right. \label{eq:d_hatx_dz}
\end{align}\]

<hr />
<h3 id="post-training-quantization-ptq-vs-quantization-aware-training-qat">Post-Training-Quantization (PTQ) vs Quantization-Aware-Training (QAT)</h3>
<hr />

<p>There are two different model quantization methodologies: <strong>Post Training Quantization (PTQ)</strong>  and <strong>Quantization Aware Training (QAT)</strong>.</p>

<p>In post training quantization (PTQ), a well-trained floating-point model is converted to a fixed point one <em>without any end-to-end training</em>. The task here is to find ranges (quantization parameters) for each weight and activation, and this is done either without any data or with only a small representative unlabelled dataset, which is often available. Since no end-to-end training is involved, it decouples model training from model quantization, which allows fast prototyping of the model.</p>

<p>Quantization aware quantization (QAT), by contrast, integrates quantization operation as part of the model, and train the quantization parameters together with its neural network parameters, where the backward flow throw the quantization operation is described in the previous section. Here we need to access full training dataset. Setting up the training pipeline that involves simulated quantization can be a difficult process and requires more effort than PTQ, but it often results in close-to-floating-point performance, sometimes even with very low bit-width.</p>

<p>In the next section, we first go over some baseline quantization range estimation methods, and then describe three techniques that boost PTQ performance. Keep in mind that some of the PTQ techniques can lead to better initialization for QAT and thus can be very relevant in the QAT context as well.</p>

<hr />
<h1 id="quantization-techniques">Quantization techniques</h1>
<hr />
<h3 id="range-estimation-methods">Range Estimation Methods</h3>
<hr />

<p>This part covers how the range (\(q_\text{min}, q_\text{max}\)) of weight and/or activation can be estimated. As noted before, \((q_\text{min}, q_\text{max})\), or equivalently \((s, z)\), uniquely determine the quantization scheme for a given fixed-point format \((b, n, p)\). All these approaches need some statistical information of the data to be quantized. For weight, this is readily available, and for activation, it can be estimated with a small set of representative input data.</p>

<p><ins><strong>Min-Max</strong></ins></p>

<p>To avoid any clipping error, we can set \(q_\text{min}\) and \(q_\text{max}\) to the min and max of the tensor to be quantized.</p>

\[\begin{align*}
q_\text{min} =\min x\\
q_\text{max} =\max x
\end{align*}\]

<p>The downside of this approach is that a large rounding error may be incurred if there are strong outliers of min/max values.</p>

<p><ins><strong>MSE</strong></ins></p>

<p>To strike the right balance between range and precision, we can instead minimize the mean-square-error (MSE) between the quantized values and the floating point ones,</p>

\[\underset{q_\text{min},q_\text{max}}{\arg\min} \|x-\hat{x}\|_F^2.\]

<p><a href="#banner_et_al_2019">[Banner et al. 2019]</a> introduced an analytical approximation of the above objective when \(x\) follows either Laplace or Gaussian distribution (or one-sided Gaussian/Laplace if \(x\) is output of ReLU activation). Alternative, a simple grid search also works.</p>

<p><ins><strong>Cross-Entropy</strong></ins></p>

<p>For model with the last layer being a softmax, we can derive quantization parameters of the last layer activation (which are logits) by minimizing the error in probability space, instead of MSE.</p>

\[\underset{q_\text{min},q_\text{max}}{\arg\min}\text{ }\texttt{CrossEntropy}(\texttt{softmax}(x),\texttt{softmax}(\hat{x}))\]

<p><ins><strong>BatchNorm</strong></ins></p>

<p>In layers with BatchNorm, we can use its learned parameters (mean \(\beta\) and std \(\beta\)) to approximate min and max as \(q_\text{min}\approx \beta - \alpha\gamma\) and \(q_\text{max} \approx \beta + \alpha\gamma\) where \(\alpha=6\) is recommended.</p>

<p>One minor detail for all the above approaches is that the value of \(q_\text{min}\) and \(q_\text{max}\) needs to tweaked to ensure that the corresponding offset \(z\) is an integer value on the grid.</p>

<blockquote>
  <p>Empirically, <strong>MSE</strong>-based method is the most robust across different models and bit-width settings, and thus is the recommended method for range estimation. For logit activation, <strong>Cross-Entropy</strong> based method can be beneficial especially in low bit-width regime (4-bit or below).</p>
</blockquote>

<hr />
<h3 id="cross-layer-equalization">Cross Layer Equalization</h3>
<hr />

<p>We have mentioned that per-tensor quantization of weight can be problematic when weights for different output channels have significantly different ranges. The issue can be side-stepped using per-channel quantization of weights, but not all hardware supports it.</p>

<p>Turns out there is a quite smart way to re-scale the weights across different layers without changing the floating model’s output, but can make it much more friendly to per-tensor quantization. The technique is called Cross Layer Equalization (CLE), proposed by <a href="#Nagel_et_al_2019">[Nagel et al. 2019]</a> and <a href="#meller_et_al_2019">[Meller et al. 2019]</a>. Let’s go over it.</p>

<p>A linear map \(f\) satisfies two properties: (1) additivity: \(f(a+b)=f(a)+f(b)\), and (2) scale equivariant \(sf(a) = f(sa)\) (scaled input leads scaled output). Some of the most commonly used nonlinear activation functions such as ReLU or leaky ReLU only give up the first property but still have the second property hold for any positive scaling factor \(s\).</p>

<p>The scale equivariant property allows us to redistribute the scaling of weights of two adjacent layers even with the activation function in the middle. In the naive scalar example, this can be seen as</p>

\[w^{(2)}f\left(w^{(1)}x\right) = sw^{(2)}f\left(\frac{w^{(1)}}{s} x\right).\]

<p>For the general case of two consecutive Conv or FC layer (weights denoted as \(W^{(1)}, W^{(2)}\)) with a scale equivariant nonlinearity in the middle (denoted as \(f\)), we can apply a scaling of \(s_i\) on the \(i^\text{th}\) input channel of \(W^{(2)}\) (\(W^{(2)}_{\cdot i}\)) and a scaling of \(1/s_i\) on the \(i^\text{th}\) output channel of \(W^{(1)}\) (\(W^{(1)}_{i \cdot}\)) without changing the output.</p>

\[\begin{align*}
&amp;W_{c_3\times c_2}^{(2)} f\left( W_{c_2\times c_1}^{(1)}x\right)\\
=&amp;\left[W_{\cdot 1}^{(2)},\ldots,W_{\cdot c_2}^{(2)}\right] f\left(\left[
  \begin{array}{c}
  W_{1\cdot}^{(1)} \\
  \vdots \\
  W_{c_2\cdot}^{(1)} \\
  \end{array}
  \right]x\right)\\
=&amp;\left[W_{\cdot 1}^{(2)},\ldots,W_{\cdot c_2}^{(2)}\right] \left[
  \begin{array}{c}
  f(W_{1\cdot}^{(1)}x) \\
  \vdots \\
  f(W_{c_2\cdot}^{(1)}x) \\
  \end{array}
  \right]\\
=&amp;\sum_{i:1\to c_2} W_{\cdot i}^{(2)}f\left(W_{i\cdot}^{(1)}x\right)\\
=&amp;\sum_{i:1\to c_2} W_{\cdot i}^{(2)} s_i f\left(\frac{W_{i\cdot}^{(1)}}{s_i}x\right)
\end{align*}\]

<p>This is illustrated in the figure below, where we take as an example two 1x1 convolutions with a ReLU in between.</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/quantization/cross_layer_equalization-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/quantization/cross_layer_equalization-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/quantization/cross_layer_equalization-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/quantization/cross_layer_equalization.png" />

  </picture>

</figure>

    </div>
</div>

<p>Intuitively, we want to redistribute the magnitude of weights between the two weight tensors in such a way that the maximum magnitude can be equalized. In other words, whenever the range of \(W^{(1)}_{i\cdot}\) is larger than that of \(W^{(2)}_{\cdot i}\), there is an incentive to down scale  \(W^{(1)}_{i\cdot}\) and up scale \(W^{(2)}_{\cdot i}\) to the point that they match in their range. With this intuition in mind, we can find the scaling factors to be applied for each channel \(i\) as (output channel of \(W^{(1)}\) and input channel of \(W^{(2)}\)):</p>

\[\begin{align*}
                &amp;\max\left| W_{\cdot i}^{(2)} s_i \right| = \max\left|\frac{W_{i\cdot}^{(1)}}{s_i}\right| \\
\Longrightarrow &amp; \max\left| W_{\cdot i}^{(2)}\right| s_i = \max\left|W_{i\cdot}^{(1)}\right|/s_i \\
\Longrightarrow &amp; s_i = \sqrt{\frac{
\max\left|W_{i\cdot}^{(1)}\right|
}{
\max\left| W_{\cdot i}^{(2)}\right|
}}.
\end{align*}\]

<p>Cross layer equalization with the scaling factor derived as above is a super effective way to boost performance of PTQ when per-tensor quantization of weight is applied. It is especially critical for models that use depth-wise separate convolution, which empirically leads to large variation in magnitude of weights from different channels. Below is Table 3 from <a href="#Nagel_et_al_2021">[Nagel et al. 2021]</a>, where it shows that applying CLE on the floating point model before PTQ can fix the PTQ performance from a completely breakdown to less than 2 percent loss compared with floating point.</p>

<div class="row mt-3">
    <div class="col-sm-3 mt-3 mt-md-0">
    </div>
    <div class="col-sm-6 mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/quantization/cle_results-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/quantization/cle_results-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/quantization/cle_results-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/quantization/cle_results.png" />

  </picture>

</figure>

    </div>
    <div class="col-sm-3 mt-3 mt-md-0">
    </div>
</div>
<div class="caption">
    Figure 3 from <a href="#Nagel_et_al_2021">[Nagel et al. 2021]</a>: PTQ ImageNet validation on MobileNetV2.  
</div>

<blockquote>
  <p>To summarize, CLE is a technique that adjust weights of a floating point model so that it becomes more friendly with per-tensor quantization of weights. Some remarks below</p>
  <ul>
    <li>In the context of PTQ, CLE is a must have whenever per-tensor quantization is applied.</li>
    <li>In the context of QAT, CLE still is a necessary model-preprocessing step to get a good initialization of the per-tensor quantization parameters.</li>
    <li>A limitation for CLE is that it cannot handle pair of layers where there is skip connection to or from the activation in the middle.</li>
    <li><a href="#meller_et_al_2019">[Meller et al. 2019]</a> proposes to apply equalization that also takes  into account the activation tensor in the middle</li>
    <li>To apply CLE to a deep model, the process is iterated for every pairs of layers (that does not have skip connection to or from the middle layer) sequentially until convergence.</li>
  </ul>
</blockquote>

<hr />
<h3 id="bias-correction">Bias Correction</h3>
<hr />

<p>Quantization of weights can lead to a shift in the mean value of the output distribution. Specifically, for a linear layer with weight \(W\) and input \(x\), the gap between the mean output of quantized weight \(\hat{W}\) and its floating point counterpart \(W\)  can be expressed as \(\mathbb{E}[\hat{W}x] - \mathbb{E}[Wx] = (W-\hat{W})\mathbb{E}[x]\). Given that \(x\) is the activation of the previous layer, \(\mathbb{E}[x]\) is often non-zero (e.g., if \(x\) is the output of the ReLU activation), and thus the gap can be non-zero.</p>

<p>This shift in mean can be easily corrected by absorbing \((W-\hat{W})\mathbb{E}[x]\) into the bias term (subtract \((W-\hat{W})\mathbb{E}[x]\) from bias) <a href="#Nagel_et_al_2019">[Nagel et al. 2019]</a>. Since \(W\) and \(\hat{W}\) are known after the quantization, we only need to estimate \(\mathbb{E}[x]\), which can come from two sources</p>

<ul>
  <li>If there is a small amount of input data, it can be used to get an empirical estimate of \(\mathbb{E}[x]\)</li>
  <li>If \(x\) is the output of a BatchNorm + ReLU layer, we can use the batch norm statistics to derive \(\mathbb{E}[x]\)</li>
</ul>

<hr />
<h3 id="adaptive-rounding">Adaptive Rounding</h3>
<hr />

<p>In PTQ, after the quantization range \([q_\text{min}, q_\text{max}]\) (or equivalently, step size \(s\) and offset \(z\)) of a weight tensor \(W\) is determined, the weight will be <strong>round</strong> to its <strong>nearest</strong> value on the fixed-point grid. Rounding to the <strong>nearest</strong> quantized value is such an apparently right operation that we don’t think twice about it. However, there is a valid reason why we may consider otherwise.</p>

<p>Let us first define a more flexible form of quantization \(\widetilde{W}\) where we can control whether to round up or down with a binary auxillary variable \(V\):</p>

\[\begin{align*}
\text{Round to nearest } \widehat{W} =&amp; s \left\lfloor W/s\right\rceil, \\
\text{Round up or down } \widetilde{W}(V) =&amp; s \left(\left\lfloor W/s\right\rfloor + V\right).
\end{align*}\]

<p>Note that we changed \(\lfloor\cdot\rceil\) into \(\lfloor\cdot\rfloor\) and ignored clamping for notational clarity. Rounding-to-nearest minimizes the mean-square-error (MSE) between the quantized values and its floating point values, i.e.,</p>

\[\widehat{W} = \min_{V\in [0, 1]^{|V|}} \left\| W - \widetilde{W}(V) \right\|_F^2.\]

<p>Instead of minimizing MSE of the quantized weight, a better target is to minimize the MSE of the activation, which reduces the effect of quantization from an input-output stand point:</p>

\[\min_{V\in [0, 1]^{|V|}} \left\|f(Wx) - f\left(\widetilde{W}(V)\bar{x}\right)\right\|_F^2.\]

<p>The method of determined whether to round up or round down by optimizing the above objective is called Adaptive Rounding or AdaRound proposed by <a href="#Nagel_et_al_2020">[Nagel et al. 2020]</a>. \(\bar{x}\) is the activation with all previous layers quantized. Note that optimization of the above objective only requires a small amount of representative input data. Please refer to <a href="#Nagel_et_al_2020">[Nagel et al. 2020]</a> for details regarding how this integer optimization problem can be solved with relaxation and annealed regularization term that encourage \(V\) to converge to 0/1.</p>

<p>Alternatively, one can use straight-through estimator (STE) to directly optimize for the quantized weight, which allows more flexible quantization beyond just rounding up or down. In the table below, we can see that AdaRound outperforms this STE approach, likely due to biased gradient of STE.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/quantization/adaround-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/quantization/adaround-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/quantization/adaround-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/quantization/adaround.png" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
    </div>
</div>
<div class="caption">
    Figure 5 from <a href="#Nagel_et_al_2020">[Nagel et al. 2020]</a>: ImageNet validation on ResNet18. 
</div>

<blockquote>
  <p>To summarize, AdaRound is an effective PTQ weight quantization technique, with the following characteristics/limitations:</p>
  <ul>
    <li>It requires the access of a small amount of representative input data (no label needed)</li>
    <li>It is only a weight quantization technique</li>
    <li>It is applied after the range (\(q_\text{min}\) and \(q_\text{max}\), or equivalently \(s\) and \(z\)) is determined.</li>
    <li>When QAT is applied, AdaRound becomes irrelevant.</li>
    <li>AdaRound absorbs bias correction in it optimization objective (In the equations above, \(f(Wx)\) should be \(f(Wx+b)\), but we ignored \(b\) for notational clarity), so whenever AdaRound is applied, bias correction is no longer needed.</li>
  </ul>
</blockquote>

<hr />
<h1 id="ptq-and-qat-best-practices">PTQ and QAT best practices</h1>
<hr />
<h3 id="ptq-pipeline-and-debugging-strategy">PTQ pipeline and debugging strategy</h3>
<hr />

<p><strong>Initial pipeline</strong></p>

<ul>
  <li>Add quantizer
    <ul>
      <li>Symmetric weight and asymmetric activation weight is recommended
        <ul>
          <li>Symmetric is preferred for weight to avoid the second term in Equation \eqref{eq:y_i}.</li>
        </ul>
      </li>
      <li>Per-tensor quantization of weight and activation</li>
    </ul>
  </li>
  <li>Range estimate for weight
    <ul>
      <li>MSE based range estimate is recommended</li>
    </ul>
  </li>
  <li>Range estimate for activation
    <ul>
      <li>MSE based range estimate is recommended</li>
    </ul>
  </li>
</ul>

<p><strong>Debug steps</strong></p>

<ul>
  <li>Check if 32-bit fixed-point model matches performance with floating-point model or not
    <ul>
      <li>bit-width of 32 gives very high precision and should match floating point model</li>
      <li>If does not match, check correctness of range learning module and quantizer.</li>
    </ul>
  </li>
  <li>Identify which one is the major cause of performance degradation: weight quantization or activation quantization
    <ul>
      <li>(A) Use bit-width of 32 for weight quantization and the targeted bit-width for activation quantization</li>
      <li>(B) Use bit-width of 32 for activation quantization and the targeted bit-width for weight quantization</li>
      <li>Once we identify which one is more problematic, we can further identify which layer(s) are the bottleneck
        <ul>
          <li>We can conduct leave-one-out analysis by quantizing all but one layer</li>
          <li>and/or add-one-only analysis by quantizing only a single layer</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>If weight quantization causes accuracy drop
    <ul>
      <li>Apply CLE as a preprocessing step before quantization</li>
      <li>Apply per-channel quantization if it is supported by the target hardware</li>
      <li>Apply AdaRound if there is a small representative unlabeled dataset available</li>
      <li>Apply bias-correction if the dataset if not available, but there is BN, which captures some data statistics</li>
    </ul>
  </li>
  <li>If activation quantization causes accuracy drop
    <ul>
      <li>Apply CLE that also takes activation range into account <a href="#meller_et_al_2019">[Meller et al. 2019]</a> before quantization</li>
      <li>Apply different range estimate methods (MSE is recommended, but for logit activation, cross-entropy based method can be applied)</li>
    </ul>
  </li>
  <li>Visualize the range/distribution for problematic tensors
    <ul>
      <li>Break down the range/distribution per-channel, per-dimension, and/or per-token (in sequence models)</li>
      <li>Set quantization to a larger bit-width for problematic layers if permitted by hardware. E.g., changing certain layers from 8-bit to 16-bit. Having heterogenous bit-width across different layers is also often referred to as mixed-precision.</li>
      <li>Apply QAT</li>
    </ul>
  </li>
</ul>

<p><strong>Final Pipeline may look like the following</strong></p>

<ul>
  <li>Apply CLE
    <ul>
      <li>Weight only CLE</li>
      <li>or CLE that takes activation into account as well</li>
    </ul>
  </li>
  <li>Add quantizer
    <ul>
      <li>Symmetric weight quantization and asymmetric activation quantization</li>
    </ul>
  </li>
  <li>Range estimate for weight
    <ul>
      <li>MSE based range estimation is recommended</li>
      <li>Min/max can also be a good choice if per-channel quantization of weight is used</li>
    </ul>
  </li>
  <li>AdaRound or bias-correction
    <ul>
      <li>Adjust quantized weight/bias given a fixed range</li>
    </ul>
  </li>
  <li>Range estimate for activation
    <ul>
      <li>MSE based range estimation is recommended</li>
      <li>Cross-entropy can also be a good choice if certain layer contains values interpreted as logits</li>
    </ul>
  </li>
</ul>

<hr />
<h3 id="qat-pipeline">QAT pipeline</h3>
<hr />

<ul>
  <li>Preprocessing of model
    <ul>
      <li>Cross-layer-equalization</li>
      <li>Absorb BatchNorm into weights and biases of the preceding layer</li>
    </ul>
  </li>
  <li>Add quantizer
    <ul>
      <li>Symmetric weight quantization and asymmetric activation quantization is recommended
        <ul>
          <li>Symmetric is preferred for weight because it voids the second term in Equation \eqref{eq:y_i}</li>
        </ul>
      </li>
      <li>Per-tensor quantization of weight and activation
        <ul>
          <li>Per-channel quantization of weight is preferable if supported by hardware</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Range estimate
    <ul>
      <li>MSE based range estimation is recommended</li>
      <li>It serves as initial quantization parameters</li>
    </ul>
  </li>
  <li>Learn quantization parameters together with neural network parameters
    <ul>
      <li>Gradient of quantization parameters can be derived from Equation \eqref{eq:x_hat_re} and are provided in Equation \eqref{eq:d_hatx_dx}, \eqref{eq:d_hatx_ds} and \eqref{eq:d_hatx_dz}.</li>
    </ul>
  </li>
</ul>

<hr />
<h1 id="treatment-of-special-layers">Treatment of special layers</h1>
<hr />

<p>For <strong>addition</strong> operation and <strong>concatenation</strong> operation, we need to make sure that two activations to be added or concatenated live on the same quantization grid, i.e., we need to tie the quantization parameters to be the same for the two activations.</p>

<p>So far we have covered Conv and MLP type of linear layers with simple activation functions. BatchNorm an be easily folded into the linear layer so we are also clear how it can be handled. These give great coverage for typical vision models, but for the quantization of sequence/NLP models that use transformer, we also need to deal with additional non-linear operators such as LayerNorm, softmax, GeLU. <a href="#Kim_et_al_2021">[Kim et al. 2021]</a> addresses this challenge by proposing polynomial approximations to these operations that can be carried out easily in fixed point arithmetic. Also see <a href="#bondarenko_et_al_2021">[Bondarenko et al. 2021]</a> that proposes a per-embedding-group quantization for PTQ which tackles the structured outliers among certain embedding dimensions of sequences.</p>

<hr />
<h1 id="references">References</h1>
<hr />
<ul>
  <li><a name="Nagel_et_al_2021"></a> <strong>[Nagel et al. 2021]</strong>  Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart van Baalen, Tijmen Blankevoort “<em><a href="https://arxiv.org/abs/2106.08295">A White Paper on Neural Network Quantization</a></em>”, Arxiv June 2021</li>
  <li><a name="Kim_et_al_2021"></a> <strong>[Kim et al. 2021]</strong> Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer “<em><a href="https://proceedings.mlr.press/v139/kim21d.html">I-BERT: Integer-only BERT Quantization</a></em>”, ICML 2021</li>
  <li><a name="bondarenko_et_al_2021"></a> <strong>[Bondarenko et al. 2021]</strong> Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort “<em><a href="https://aclanthology.org/2021.emnlp-main.627/">Understanding and Overcoming the Challenges of Efficient Transformer Quantization</a></em>”, EMNLP 2021 <a href="https://github.com/qualcomm-ai-research/transformer-quantization">code</a></li>
  <li><a name="Nagel_et_al_2020"></a> <strong>[Nagel et al. 2020]</strong> Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, Tijmen Blankevoort “<em><a href="http://proceedings.mlr.press/v119/nagel20a.html">Up or Down? Adaptive Rounding for Post-Training Quantization</a></em>”, ICML 2020</li>
  <li><a name="Nagel_et_al_2019"></a> <strong>[Nagel et al. 2019]</strong> Markus Nagel, Mart van Baalen, Tijmen Blankevoort, Max Welling, “<em><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Nagel_Data-Free_Quantization_Through_Weight_Equalization_and_Bias_Correction_ICCV_2019_paper.html">Data-Free Quantization Through Weight Equalization and Bias Correction</a></em>”, ICCV 2019</li>
  <li><a name="meller_et_al_2019"></a> <strong>[Meller et al. 2019]</strong> Eldad Meller, Alexander Finkelstein, Uri Almog, Mark Grobman, “<em><a href="http://proceedings.mlr.press/v97/meller19a.html">Same, Same But Different: Recovering Neural Network Quantization Error Through Weight Factorization</a></em>”, ICML 2019</li>
  <li><a name="banner_et_al_2019"></a> <strong>[Banner et al. 2019]</strong> Ron Banner, Yury Nahshan, Daniel Soudry “<em><a href="https://proceedings.neurips.cc/paper/2019/hash/c0a62e133894cdce435bcb4a5df1db2d-Abstract.html">Post training 4-bit quantization of convolutional networks for rapid-deployment</a></em>”, NeurIPS 2019</li>
  <li><a name="Jacob_et_al_2018"></a> <strong>[Jacob et al. 2018]</strong> Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, Dmitry Kalenichenko, <em>“<a href="https://openaccess.thecvf.com/content_cvpr_2018/html/Jacob_Quantization_and_Training_CVPR_2018_paper.html">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</a>“</em>, CVPR 2018</li>
  <li><a name="Krishnamoorthi_2018"></a> <strong>[Krishnamoorthi 2018]</strong> Raghuraman Krishnamoorthi, “<em><a href="https://arxiv.org/abs/1806.08342">Quantizing deep convolutional networks for efficient inference: A whitepaper
</a></em>”, Arxiv 2018</li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[how to properly quantize neural networks for efficient hardware inference?]]></summary></entry><entry><title type="html">Enforcing Lipschitz Constant in Neural Network</title><link href="https://yyang768osu.github.io/blog/2021/enforcing-lipschitz-constant-in-neural-network/" rel="alternate" type="text/html" title="Enforcing Lipschitz Constant in Neural Network" /><published>2021-04-03T00:00:00+00:00</published><updated>2021-04-03T00:00:00+00:00</updated><id>https://yyang768osu.github.io/blog/2021/enforcing-lipschitz-constant-in-neural-network</id><content type="html" xml:base="https://yyang768osu.github.io/blog/2021/enforcing-lipschitz-constant-in-neural-network/"><![CDATA[<p>A function \(g(x)\) is Lipschitz continuous if there exists a constant \(L\) such that \(\|g(x_1) - g(x_2)\| &lt; L \|x_1 - x_2\|\) for any \(x_1\) and \(x_2\) in its domain. \(L\) is referred to as a Lipschitz constant of \(g\). The need to enforce a certain Lipschitz constant of neural networks arises in many cases, with some examples listed below. Here we introduce a common technique used in many existing literatures.</p>

<ul>
  <li>Guarantee invertibility in normalizing flows build with residual blocks
    <ul>
      <li><a href="https://arxiv.org/abs/1811.00995">iResNet(ICML2019)</a></li>
    </ul>
  </li>
  <li>Discriminator regularization in GAN training
    <ul>
      <li><a href="https://arxiv.org/abs/1701.07875">Wasserstein-GAN(ICML2017)</a></li>
      <li><a href="https://arxiv.org/abs/1802.05957">SpectralNormalization(ICLR2018)</a></li>
    </ul>
  </li>
  <li>Improve network robustness against adversarial perturbations
    <ul>
      <li><a href="https://arxiv.org/abs/1802.04034">Lipschitz-margin-training(NIPS2018)</a></li>
    </ul>
  </li>
</ul>

<p>A small note before we proceed: Lipschitz continuous/constant is defined with respect to a choice of the norm \(\|\cdot\|\). Here we focus on 2-norm.</p>

<h2 id="lipschitz-constant-vs-spectral-norm-of-matrices">Lipschitz constant vs spectral norm of matrices</h2>

<p>Deep neural networks are typically build with interleaved linear layers (such as Conv, TConv, Pooling) and nonlinear activations (such as ReLU, sigmoid). The Lipschitz constant of most activation functions are either constant or easy to control, so we will only focus on linear operations. Linear operations in general can be expressed as in the form of matrix-vector product \(y = g(x) = Wx\) where \(W\) denotes a matrix. In this case, the smallest Lipschitz constant of \(g\) can be expressed as</p>

<p>\begin{equation}
\label{eq:lipconst}
\min_{x_1, x_2, x_1\not=x_2} \frac{
||g(x_1)-g(x_2)||
}{
||x_1 - x_2||
}
=
\min_{||v||\not=0}\frac{
||Wv||
}{
||v||
}
=
\min_{||v||=1}
||Wv||.
\end{equation}</p>

<p>The last term is also known as the <em>spectral norm</em> of matrix \(W\). Let us express \(W\) as its singular-value-decomposition \(U\Sigma V^T\), then we can see that the spectral norm of \(W\) is its maximum singular value, denoted as \(\sigma_1\). The maximum singular value of \(W\) is also the maximum eigenvalue of \(M\triangleq W^TW\) given that eigenvalues of \(W^TW\) is square of singular values of \(W\): \(M=V\Sigma U^TU\Sigma V^T=V\Sigma^2V^T=V\Lambda V^T\).</p>

<p>Now we know that obtaining the best Lipschitz constant of a linear operations amounts to finding the dominant singular value of its matrix representation \(W\), or dominant eigenvalue of \(M\triangleq W^TW\). Next let us introduce an iterative algorithm that can find it.</p>

<h2 id="power-method-aka-von-mises-iteration">Power method (aka Von Mises iteration)</h2>

<p>Power method finds the maximum eigenvalue of a matrix \(M\) using the following iteration:</p>

\[\begin{align*}
&amp;\text{start with a random vector }v^{(0)} \\
&amp;\text{for }k=1, 2, \ldots, \\
&amp;v^{(k)} = \frac{
M v^{(k-1)}
}{
||M v^{(k-1)}||
}
\end{align*}\]

<p>Claim: \(\|M v^{(k)}\|\) converges to the maximum eigen-value of \(M\) as \(k\) approaches infinity.</p>

<p>To show it, let us write the initial vector \(v^{(0)}\) as a linear combinations of eigen-vectors of \(M\): \(v^{(0)}=\sum_{i}\alpha_i v_i\), and expand the iterative formula as</p>

\[\begin{align*}
&amp;v^{(k)} = \frac{
M v^{(k-1)}
}{
||M v^{(k-1)}||
}
= \frac{
M^2 v^{(k-2)}
}{
||M^2v^{(k-2)}||
}=\ldots
= \frac{
M^k v^{(0)}
}{
||M^kv^{(0)}||
}
\\
&amp;M^k v^{(0)} = M^k \sum_{i} \alpha_i v_i = \sum_{i} \alpha_i M^k v_i =\sum_{i}\alpha_i \lambda_i^k v_i = \alpha_1\lambda_1^k
\left(
v_1 + \sum_{i&gt;1}\underbrace{\frac{\alpha_i}{\alpha_1}\left(\frac{\lambda_i}{\lambda_1}\right)^k}_{\to 0 \text{ as } k\to\infty} v_i
\right).
\end{align*}\]

<p>From the last equation we know that \(v^{(k)}\) converges to the dominant eigen-vector \(v_1\) of \(M\) up to a sign difference, and similarly \(Mv^{(k)}\) converges to the maximum eigen-value \(\sigma_1\) of \(M\).</p>

\[\begin{align*}
v^{(k)}\to\left\{\begin{array}{ll}
v_1 &amp; \text{if }\alpha_1&gt;0\\
-v_1 &amp; \text{if }\alpha_1&lt;0
\end{array}\right., \text{ as }k\to\infty
\end{align*}\]

<h2 id="compute-power-iteration-through-auto-differentiation">Compute power iteration through auto-differentiation</h2>

<p>From last section we know that the maximum singular value can be computed if we carry out the following iteration procedure:</p>

\[\begin{align*}
 v^{(k-1)} \Longrightarrow \underbrace{\tilde{v}^{(k)}=W^TWv^{(k-1)}}_{\text{step 1}} \Longrightarrow \underbrace{v^{(k)}=\tilde{v}^{(k)}/||\tilde{v}^{(k)}||}_{\text{step 2}} \Longrightarrow\ldots
\end{align*}\]

<p>While it is easy to compute vector norm as done in step 2, it is not immediately clear how to easily compute \(W^TWv^{(k-1)}\) in step 1, since expressing \(W\) explicitly for a general linear layer can be involved. For instance, for a 2D convolution operation, expressing it in the matrix-vector product form requires unpacking the convolution kernel into a doubly Toeplitz matrix. We know that \(Wv^{(k-1)}\) is just the output of the linear operator \(g\) when \(v^{(k-1)}\) is used as input, but seemingly there is no easy way to multiply by \(W^T\) without knowing \(W\) explicitly.</p>

<p>Here’s the trick: we can express \(W^TWx\) as the derivative of another another function and compute it with auto-differentiation.</p>

\[\begin{align*}
W^TW x = \frac{1}{2}\frac{\partial x^TW^TWx}{\partial x} = \frac{1}{2}\frac{\partial ||Wx||^2}{\partial x} =\frac{\partial \frac{1}{2}||g(x)||^2}{\partial x}
\end{align*}\]

<p>We can then modify the iteration procedure as</p>

\[\begin{align*}
 v^{(k-1)} \Longrightarrow \underbrace{
\tilde{v}^{(k)} = \frac{
\partial\frac{1}{2}||g(v^{v^{(k-1)}})||^2
}{
\partial v^{(k-1)}
}
 }_{\text{step 1}} \Longrightarrow \underbrace{v^{(k)}=\tilde{v}^{(k)}/||\tilde{v}^{(k)}||}_{\text{step 2}} \Longrightarrow\ldots
\end{align*}\]

<p>Based on last section, \(\sqrt{\|\tilde{v}^{(k)}\|}\) yields an estimate of the dominant singular value of \(M\), which is the Lipschitz constant of the linear operator \(g\).
In PyTorch, step 1 can be calculated using <a href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad">torch.atuograd.grad</a>.</p>

<p>It should not be surprising that the above iteration procedure converges to maximum singular value of \(W\) – it is simply the gradient ascent with Equation \eqref{eq:lipconst} as the optimization objective.</p>

<h2 id="enforce-lipschitz-constant-c-during-training">Enforce Lipschitz constant \(c\) during training</h2>

<p>It is easy to see that the Lipschitz constant of \(a\times g(\cdot)\) is \(a\) times the Lipschitz constant of \(g(\cdot)\), or more precisely, \(\text{Lip}(ag) = a\text{Lip}(g)\). To enforce the Lipschitz constant of an operator to be some target value \(c\), we just need to normalize the output the operator by \(c/\text{Lip}(g)\).</p>

<p>The power iteration procedure itself can be amortized and blended into the optimization step of the network training, in which case the training loop can be expressed as</p>

\[\begin{align*}
&amp;\text{for step }k=1, \ldots:\\
&amp;v = v/||v||\\
&amp;v = \frac{
\partial\frac{1}{2}||g(v)||^2
}{
\partial v 
}\\
&amp;\sigma = \sqrt{||v||}\\
&amp;\text{set the normalization scale of output of }g\text{ as }\frac{c}{\sigma}\\
&amp;\text{the rest of the training step}.
\end{align*}\]]]></content><author><name></name></author><summary type="html"><![CDATA[how to enforce lipschitz constraint in neural networks?]]></summary></entry><entry><title type="html">Langevin Dynamics for Bayesian Inference</title><link href="https://yyang768osu.github.io/blog/2020/langevin-dynamics/" rel="alternate" type="text/html" title="Langevin Dynamics for Bayesian Inference" /><published>2020-09-06T00:00:00+00:00</published><updated>2020-09-06T00:00:00+00:00</updated><id>https://yyang768osu.github.io/blog/2020/langevin-dynamics</id><content type="html" xml:base="https://yyang768osu.github.io/blog/2020/langevin-dynamics/"><![CDATA[<p>In this post we visit some technical details centered around Langevin Dynamics in the context of stochastic Bayesian learning, assuming minimal background on conventional calculus and Brownian motion. Starting with quadratic variation, we gradually show how Ito’s Lemma and Fokker-Planck equation can be derived. Using Fokker-Planck equation, it is revealed that an Langevian dynamic can be used as a MCMC method to generate samples from an un-normalized distribution. Lastly, <a href="https://www.ics.uci.edu/~welling/publications/papers/stoclangevin_v6.pdf">stochastic gradient Langevin dynamics</a> method is discussed.</p>

<p>The following materials are taken as references:</p>

<ul>
  <li><a href="https://www.math.ucdavis.edu/~hunter/m280_09/ch5.pdf">UC-Davis Lecture Notes on Applied Mathematics</a></li>
  <li><a href="https://www.youtube.com/watch?v=PPl-7_RL0Ko">MIT Topics in Mathematics with Applications in Finance Lecture 17: Stochastic Processes II</a></li>
  <li><a href="https://www.youtube.com/watch?v=Z5yRMMVUC5w">MIT Topics in Mathematics with Applications in Finance Lecture 18: Itō Calculus</a></li>
</ul>

<h2 id="quadratic-variation">Quadratic Variation</h2>
<p>For Brownian motion \(B_t\), 
we know that \(B\left(\frac{i+1}{N}T\right) - B\left(\frac{i}{N}T\right)\) for different index \(i\) are i.i.d. with distribution \(\mathcal{N}\left(0, \frac{T}{N}\right)\). The following holds by strong law of large numbers.</p>

\[\begin{align*}
\lim_{N\to\infty}\sum_{i=1}^N \left(B\left(\frac{i+1}{N}T\right) - B\left(\frac{i}{N}T\right)\right)^2&amp;=T \text{ a.s.} \\
\end{align*}\]

<p>The above can be written in differential form as</p>

\[\begin{align*}
\int (dB)^2 &amp;= \int dt\\
(dB)^2 &amp;= dt\\
\end{align*}\]

<p>which is known as quadratic variation. This means that the second order term of Taylor expansion involving \(B_t\) scales as \(O(t)\) instead of \(o(t)\), the implication of which is detailed in Ito’s Lemma below.</p>

<h2 id="itos-lemma">Ito’s Lemma</h2>

<p>Suppose we want to compute \(f(B_t)\) for some smooth function \(f\). By Taylor expansion, the infinitesimal difference can be expressed as</p>

\[\begin{align*}
f(B_{t+\Delta t}) - f(B_t) &amp;= f'(B_t) (B_{t+\Delta t} - B_t) + \frac{f''(B_t)}{2}\left(B_{t+\Delta t}-B_t\right)^2 \\
                  \text{(differential form) }      df &amp;= f'(B_t) dB_t + \frac{f''(B_t)}{2}\left(dB_t\right)^2 \\
                  \text{(quadratic variation) }      df &amp;= f'(B_t) dB_t + \frac{f''(B_t)}{2} dt\\
                       \frac{df}{dt} &amp;= f'(B_t) \frac{dB_t}{dt} \color{red}{+ \frac{f''(B_t)}{2}}
\end{align*}\]

<p>The above equation is a naive version of Ito’s Lemma, the basis of Ito’s calculus. Note how it differs from conventional calculus by having the second term in red, as a direct consequence of quadratic variation.</p>

<p>Let us now look at a more advanced version of Ito’s Lemma, with the goal of obtaining the differential form of \(f(x_t, t)\) where \(x_t\) is a stochastic process defined with the following stochastic differential equation</p>

\[\begin{align*}
dx_t = \mu(x_t)dt + \sigma dB_t
\end{align*}\]

<p>Similarly as before, let’s apply Taylor expansion on the infinitesimal difference of \(f\)</p>

\[\begin{align*}
f(x+\Delta x, t+\Delta t) - f(x, t) &amp;= \frac{\partial f}{\partial t} \Delta t + \frac{\partial f}{\partial x} \Delta x + \frac{1}{2}\left[
\frac{\partial^2 f}{\partial t^2}\Delta t^2 + 2\frac{\partial^2 f}{\partial t \partial x} \Delta t \Delta x + \frac{\partial^2 f}{\partial x^2}(\Delta x)^2
\right] \\
\text{(differential form) } d f &amp;= \frac{\partial f}{\partial t}dt + \frac{\partial f}{\partial x} dx_t +\frac{1}{2}\left[
o(dt) + o(dt) + \frac{\partial^2 f}{\partial x^2}(dx_t)^2
\right] \\
\text{(substitute $dx_t$) } d f &amp;= \frac{\partial f}{\partial t}dt + \frac{\partial f}{\partial x} (\mu(x_t)dt +\sigma dB_t) +\frac{1}{2}
\frac{\partial^2 f}{\partial x^2}(\mu(x_t)^2(dt)^2 + 2\mu(x_t)\sigma dt dB_t + \sigma^2 (dB_t)^2)
\\
\text{(quadratic variation) } d f &amp;= \left(\frac{\partial f}{\partial t} + \mu(x_t)\frac{\partial f}{\partial x} + \color{red}{\frac{1}{2}\sigma^2\frac{\partial^2 f}{\partial x^2}}\right)dt + \sigma\frac{\partial f}{\partial x} dB_t
\end{align*}\]

<p>Again, the red term highlights the difference to conventional calculus. In the special when \(f\) is not a function of \(t\), the above can be reduced to</p>

\[\begin{align*}
 d f &amp;= \left(\mu(x_t)f'(x) + \color{red}{\frac{1}{2}\sigma^2 f''(x)}\right)dt + \sigma f'(x) dB_t,
\end{align*}\]

<p>which is used in the derivation of Fokker-Planck equation in the next section.</p>

<h2 id="fokker-planck-equation">Fokker-Planck equation</h2>

<p>For a stochastic process \(x\) that is defined as \(dx_t = \mu(x_t) dt + \sigma dB_t\), we are interested in how the distribution \(p_t\) of \(x_t\) evolves over time. For an arbitrary smooth function \(f\), the following holds</p>

\[\begin{align*}
\frac{d}{dt}\mathbb{E}\left[f(x_t)\right] = \left\{
\begin{array}{ll}
\int f(x) \frac{d}{dt}p_t(x) dx &amp; \text{ $f(x)$ viewed as a function of $x$ sampled from $p_t$} \\
\mathbb{E}\left[\frac{d}{dt}f(x_t)\right] &amp; \text{ $f(x_t)$ viewed as a function of stochastic process $x_t$}\\
\end{array}
\right.
\end{align*}\]

<p>The second expression can be evaluated with Ito’s Lemma.</p>

\[\begin{align*}
&amp;\mathbb{E}\left[\frac{d}{dt}f(x_t)\right]\\
\text{(Ito's Lemma) }=&amp;\mathbb{E}\left[\mu(x_t)f'(x_t)+\frac{1}{2}\sigma^2f''(x_t) + \sigma f'(x_t) \frac{dB_t}{dt}\right] \\
\text{($dB_t$ has mean $0$) }=&amp;\mathbb{E}\left[\mu(x_t)f'(x_t)+\frac{1}{2}\sigma^2f''(x_t) \right] \\
\text{(using $x_t\sim p_t$) }=&amp;\int\left[\mu(x)f'(x)+\frac{1}{2}\sigma^2f''(x) \right]p_t(x)dx \\
\text{(integration by part) }=&amp;-\int f(x)\frac{\partial (\mu(x)p_t(x)) }{\partial x}dx+\frac{1}{2}\sigma^2\int f(x)\frac{\partial^2 p_t(x)}{\partial x^2} p_t(x)dx
\end{align*}\]

<p>Combining the above two and cancelling out the arbitrary function \(f\), we obtain Fokker-Planck equation below.</p>

\[\begin{align*}
\frac{d}{dt}p_t = -\frac{\partial}{\partial x}\left(\mu(x)p_t(x)\right)+\frac{1}{2}\sigma^2\frac{\partial^2}{\partial x^2}p_t(x)
\end{align*}\]

<h2 id="langevin-dynamics">Langevin Dynamics</h2>

<p>let \(\mu(x) = -u'(x)\) for some function \(u(x)\), then the corresponding stochastic process is defined as \(dx_t = -u'(x_t) dt + \sigma dB_t\), often referred as over-damped Langevin process. Using Fokker-Planck equation, we know that</p>

\[\begin{align*}
p(x) \propto e^{-2/\sigma^2 u(x)}
\end{align*}\]

<p>is the stationary distribution of \(x_t\).</p>

\[\begin{align*}
&amp;\frac{\partial}{\partial x}\left(u'(x)p(x)\right)+\frac{1}{2}\sigma^2\frac{\partial^2}{\partial x^2}p(x) \\
=&amp;u''(x)p(x)-\frac{2}{\sigma^2}(u'(x))^2 p(x) + \frac{1}{2}\sigma^2\left(-\frac{2}{\sigma^2}u''(x)p(x) + \frac{4}{\sigma^4}(u'(x))^2 p(x)\right)=0
\end{align*}\]

<h3 id="langevin-mcmc">Langevin MCMC</h3>

<p>The fact that Langevin process \(dx_t = -u'(x_t) dt + \sigma dB_t\) converges to a stationary distribution \(p(x) \propto e^{-2/\sigma^2 u(x)}\) lends itself as a suitable Markov chain Monte Carlo method. Specifically, to obtain samples from a un-normalized density function \(\bar{p}(x)\), we just need to run the following Langevin process from a random starting point till it reaches steady state distribution</p>

\[\begin{align*}
dx_t = \nabla_x \log \bar{p}(x) dt + \sqrt{2} dB_t
\end{align*}\]

<p>Discretized sample path of Langevin process can be generated with Euler method</p>

\[\begin{align*}
x_{k+1}  = x_k  + \nabla_x \log \bar{p}(x_k) \epsilon + \sqrt{2\epsilon}\xi_k
\end{align*}\]

<p>Since the discretization is only an approximation to the original continuous stochastic process, it does not in itself lead to desired stationary distribution (unless \(\epsilon\) becomes infinitesimal) and thus should be corrected by Metropolis-Hastings to enforce detailed balance condition.</p>

<p>One lingering question is: does the discretization of Langevin dynamics satisfy detailed balance equation in \(\epsilon\to0\) asymptote? The fact that it converges to a desirable distribution does not indicate that it is a time-reversible Markov chain. Even thought it is claimed by some source that the asymptotic acceptance ratio approaches 1, I was not able to show that it is the case and is stuck at the following derivation.</p>

\[\begin{align*}
&amp;\frac{\bar{p}(x)P(x\to x')}{\bar{p}(x')P(x'\to x)} = \frac{
\bar{p}(x)\mathcal{N}\left(x'-x-\nabla_x \bar{p}(x)\tau|0, 2\tau\right)
}{
\bar{p}(x')\mathcal{N}\left(x-x'-\nabla_x \bar{p}(x')\tau|0, 2\tau\right)
}\\
=&amp; 
\frac{
\bar{p}(x)e^{(x'-x)\nabla_x \bar{p}(x)/2 + o(\tau)}
}{
\bar{p}(x')e^{(x-x')\nabla_x \bar{p}(x')/2 + o(\tau)} 
}
=
\frac{
\bar{p}(x)e^{(x'-x)\nabla_x \frac{\bar{p}(x)+\bar{p}(x')}{2} + o(\tau)}
}{
\bar{p}(x') 
}
\end{align*}\]

<h3 id="relevance-to-bayesian-inference">Relevance to Bayesian Inference</h3>

<p>In Bayesian inference we deal with a prior distribution \(p_\text{prior}(\theta)\) for some latent parameter \(\theta\) and a likelihood term \(p_\text{likelihood}(\mathcal{D}\|\theta)\) of the dataset \(\mathcal{D}\) given the latent parameter, and the goal is to obtain samples according to the posterior probability \(p_\text{post}(\theta\|\mathcal{D}) = p_\text{prior}(\theta)p_\text{likelihood}(\mathcal{D}\|\theta)/p(\mathcal{D})\). Since the constant marginal likelihood term \(p(\mathcal{D})=\int p_\text{likelihood}(\mathcal{D}\|\theta)p_\text{prior}(\theta)d\theta\) is often intractable, we are left with a un-normalized poster probability \(p_\text{post}\propto p_\text{prior}p_\text{likelihood}\). To sample from it, we can simply construct and run the following stochastic process</p>

\[\begin{align*}
d\theta_t = \left(\nabla_\theta \log p_\text{prior}(\theta) + \nabla_\theta \log p_\text{likelihood}(\mathcal{D}|\theta)\right) dt + \sqrt{2} dB_t
\end{align*}\]

<p>Hereafter we use the notation of \(x\) to indicate elements in the dataset \(x\in\mathcal{D}\), \(\theta\) to denote the hidden parameter for which we want to conduct Bayesian inference, and drop the subscript to different \(p\) as they can be differentiated by their arguments.</p>

<h2 id="stochastic-gradient-langevin-dynamics-sgld">Stochastic Gradient Langevin Dynamics (SGLD)</h2>

<p>Discretizing Langevin dynamics with step size of \(\epsilon_t\) leads to the following update rule</p>

\[\begin{align*}
\Delta \theta = \epsilon_t \left(\nabla_\theta \log p(\theta) + \nabla_\theta \log p(\mathcal{D}|\theta)\right) + \sqrt{2 \epsilon_t} \xi_t, \text{ where }\xi_t\sim\mathcal{N}(0, 1)
\end{align*}\]

<p>If we have \(\sum_t\epsilon_t = \infty\) and \(\sum_t\epsilon^2 &lt;\infty\) then asymptotically the discretization error will become negligible and the update rule approaches the corresponding Langevin dyanmics, resulting in a sequence of \(\theta_t\) that converges to the posterior distribution \(p(\theta\|\mathcal{D})\).</p>

<p>An interesting and clever observation made by <a href="https://www.ics.uci.edu/~welling/publications/papers/stoclangevin_v6.pdf">stochastic gradient Langevin dynamics</a> paper is that the convergence will hold even if we use mini-batches of the data to estimate the gradient of \(\nabla_\theta \log p(\mathcal{D}\|\theta)\).</p>

\[\begin{align*}
\nabla_\theta \log p(\mathcal{D}|\theta) \approx \frac{N}{n}\sum_{i=1}^n\nabla_\theta \log p(x_{t,i}|\theta)
\end{align*}\]

<p>The insight is that the stochastic error introduced from using mini-batches instead of the whole dataset dies out much faster than the added Gaussian noise as the \(\epsilon_t\) decreases, so it does not change the asymptotical behavior of the update rule. Specifically, the randomness coming from the stochastic estimate of \(\nabla_\theta \log p(\mathcal{D}\|\theta)\) has a variance that scales as \(\epsilon_t^2\) since it is multiplied with \(\epsilon_t\). In comparison, the variance of the added Gaussian noise scales linearly as \(\epsilon_t\).</p>

\[\begin{align*}
\Delta \theta =\underbrace{ \underbrace{ \underbrace{\epsilon_t \frac{N}{n}\sum_{i=1}^n\nabla_\theta \log p(x_{t, i}|\theta)}_{\text{gradient step towards ML target}} +\epsilon_t \nabla_\theta \log p(\theta)}_{\text{gradient step towards MAP target}} + \sqrt{2 \epsilon_t} \xi_t}_{\text{stochastic gradient Langevin dynamics for posterior sampling}} , \text{ where }\xi_t\sim\mathcal{N}(0, 1)
\end{align*}\]

<p>Given that stochastic Langevin dynamics converges to the desired distribution as \(\epsilon_t\to0\), we do not need to carry out Metropolis-Hastings to reject samples. This is crucial in simplifying the algorithm, since evaluation of rejection/acceptance rate is computed at every step and it depends on the evaluation of \(p(\theta)p(\mathcal{D}\|\theta)\) which can only be computed after traversing the whole dataset.</p>

<p>As a closing remark, if we use the posterior sampling for the estimation of the expectation of some function \(f\), it is recommended in <a href="https://www.ics.uci.edu/~welling/publications/papers/stoclangevin_v6.pdf">stochastic gradient Langevin dynamics</a> that the following equation be used.</p>

\[\begin{align*}
\mathbb{E}[f(\theta)] = \frac{\sum_t \epsilon_t f(\theta_t)}{\sum_t \epsilon_t}
\end{align*}\]

<p>with the intuition that each \(\theta_t\) contributes an effective sample size proportional to \(\epsilon_t\).</p>]]></content><author><name></name></author><summary type="html"><![CDATA[stochastic differential equation, Fokker Plank equation, and their connections to Bayesian inference]]></summary></entry><entry><title type="html">Understanding and Implementing Asymmetric Numeral System (ANS)</title><link href="https://yyang768osu.github.io/blog/2020/understanding-and-implementing-ans/" rel="alternate" type="text/html" title="Understanding and Implementing Asymmetric Numeral System (ANS)" /><published>2020-06-26T00:00:00+00:00</published><updated>2020-06-26T00:00:00+00:00</updated><id>https://yyang768osu.github.io/blog/2020/understanding-and-implementing-ans</id><content type="html" xml:base="https://yyang768osu.github.io/blog/2020/understanding-and-implementing-ans/"><![CDATA[<p>Denote an alphabet with \(N\) different symbols as \(\mathcal{A}=\{0, 1, \ldots, N-1\}\). Let us consider a source coding algorithm where a sequence of these symbols are encoded into a sequence of bits, which is represented by an integer \(s\), and assume that we can decode each symbol sequentially by breaking down \(s\) into one symbol \(x\in\mathcal{A}\) and a new integer \(s'\in\mathbb{N}\) capturing information of the remaining symbols. The encoding and decoding operation can be represented by the following <code class="language-plaintext highlighter-rouge">push</code> and <code class="language-plaintext highlighter-rouge">pop</code> operation:</p>

\[\begin{align*}
\text{encode/push  }e:&amp; \mathbb{N}\times\mathcal{A} \to \mathbb{N}\\
\text{decode/pop   }d:&amp; \mathbb{N} \to \mathbb{N}\times\mathcal{A}
\end{align*}\]

<p>There are two design goals for such a codec:</p>
<ol>
  <li>validity: for valid encoding and decoding, we want to make sure that \(e\) and \(d\) are bijections and inverse of each other (\(e=d^{-1}\)).</li>
  <li>efficiency: for coding efficiency, we want the final codeword length to approach the entropy of the data source</li>
</ol>

<p>Let us focus on the decoding process \(d\) and denote the mapping from an integer \(s\) using \(d\) as \(d(s) = s', x\) where \(x\in\mathcal{A}\). From information theory we know that, the information content (the amount of surprise) of the event of encountering \(x\) with probability \(P(x)\) can be expressed as \(-\log P(x)\). Then, to optimal performance we expect that the codeword length used to represent \(x\) to be roughly \(1/\log(P(x))\). In other words, in an efficient encoding algorithm, the number of bits in \(s\) (\(\log(s)\)) to be \(1/\log(P(x))\) more than that of \(s'\):</p>

\[\begin{align*}
\log(s) - \log(s') \approx \frac{1}{\log P(x)}\text{ for }d(s) = x, s'.
\end{align*}\]

<p>Expressed in another way</p>

\[\begin{align*}
\frac{s'}{s} = P(x)\text{ for }d(s) = x, s'.
\end{align*}\]

<p>The question is: how can we design a bijective mapping from \(\mathbb{N}\) and \(\mathbb{N}\times\mathcal{A}\) satisfying the above goal? Here’s the core idea of asymmetric numerical system: let us assume that we have access to a function that maps each of the number in \(\mathbb{N}\) into one of the symbols in \(\mathcal{A}\), denoted as \(h:\mathbb{N}\to\mathcal{A}\) with the property that for any integer \(s\) and symbol \(x\), there are roughly \(P(x)\times s\) numbers below \(s\) with symbol \(x\). Put more precisely,</p>

\[\begin{align*}
\frac{|\{n\in\mathbb{N}, n&lt;s, h(n) = x\}|}{s} \approx P(x) \text{ for any }s\in\mathbb{N}\text{ and }x\in\mathcal{A}.
\end{align*}\]

<p>With such a mapping \(h\) available, we can define the bijective decoder mapping \(d\) to be</p>

\[\begin{align*}
d(s)=&amp;s',x\text{ where} \\
s'=&amp;\left|\left\{n\in\mathbb{N},n&lt;s,h(n)=h(s)\right\}\right|,\\
x=&amp;h(s).
\end{align*}\]

<p>and it is easy to check that our two design goals are satisfied, and now we have shifted our task to finding such a labeling function \(h\) such that it leads to easy computation of \(d\) and \(e\).</p>

<h2 id="mapping-of-natural-numbers-to-symbols-h">Mapping of natural numbers to symbols (\(h\))</h2>

<p>In r-ANS (range-ANS) design, the pmf \(P:\mathcal{A}\to[0, 1]\) is quantized into integers \(p(x)\) where</p>

\[\begin{align*}
&amp;\sum_{x\in\mathcal{A}}p(x)=2^r\\
&amp;p(x)/2^r\approx P(x). 
\end{align*}\]

<p>The mapping \(h\) is design as the following: we divide the natural numbers into chunks with length \(2^r\). Within each chunk, start with symbol \(0\in\mathcal{A}\), we map the first \(p(0)\) numbers to \(0\); then the subsequent \(p(1)\) numbers are mapped to \(1\), so on and so forth.</p>

<p>Let us define \(c:\mathcal{A} \to \mathcal{N}\) with \(c(x)=\sum_{a\in\mathcal{A}, a&lt;x}p(a)\). Then the number from \(c(x)\) to \(c(x)+p(x)\) within a length \(2^r\) chunk is labeled as \(x\).</p>

<p>With this mapping, can express \(d\) and \(e\) into the following arithmetics that are easy to compute:</p>

\[\begin{align*}
d(s) =&amp; p(x)\times(s//2^r) + (s\text{ mod }2^r-c(x)), x\triangleq h(s) \\
e(s', x) =&amp; 2^r \times (s'//p(x)) + (s'\text{ mod }p(x) + c(x))
\end{align*}\]

<p>with this comes the first implementation of ANS</p>

<h2 id="ans-without-rescaling-flawed-version">ANS without rescaling (flawed version)</h2>

<h3 id="encoder">Encoder</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ans_encoder</span><span class="p">(</span><span class="n">symbols</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">symbols</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span> <span class="o">*</span> <span class="p">(</span><span class="n">s</span> <span class="o">//</span> <span class="n">p</span><span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="o">+</span> \
            <span class="n">s</span> <span class="o">%</span> <span class="n">p</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">+</span> <span class="n">c</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">s</span>
</code></pre></div></div>

<h3 id="decoder">Decoder</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ans_decoder</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">h</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">s</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span>
        <span class="c1"># this loop can be improved by binary search
</span>        <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">c</span><span class="p">))):</span>
            <span class="k">if</span> <span class="n">s</span> <span class="o">&gt;=</span> <span class="n">c</span><span class="p">[</span><span class="n">a</span><span class="p">]:</span>
                <span class="k">return</span> <span class="n">a</span>
    <span class="n">decoded_symbols</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">while</span> <span class="n">s</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">decoded_symbols</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">p</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">s</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span><span class="p">)</span> <span class="o">+</span> \
            <span class="n">s</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="n">r</span><span class="p">)</span> <span class="o">-</span> <span class="n">c</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">decoded_symbols</span><span class="p">)</span>
</code></pre></div></div>

<p>Running this encoder decoder pair through tests, one will realize that there is a small issue with the encoder and decoder function \(e\) and \(d\). Specifically, if both \(s'\) and \(x\) are \(0\), then \(s=e(0, 0)=0\). This means that any front-loaded \(0\)-sequence will be just be coded into \(0\), and decoder has no ways of knowing how many \(0\) symbols are there in the front of the sequence, if any! To solve this issue, we need to additionally guarantee that \(e\) results in strictly increasing integer. A fixed version is provided in the next section.</p>

<h2 id="ans-without-rescaling-correct-version">ANS without rescaling (correct version)</h2>

<h3 id="encoder-1">Encoder</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ans_encoder</span><span class="p">(</span><span class="n">symbols</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
    <span class="s">""" ANS encoder (no rescaling)

    Parameters
    ----------
    symbols : list of int
        list of input symbols represented by index
        value should not be larger than len(p)
    p : list of int
        quantized pmf of all input alphabet, sum(p) == 2 ** r
    c : list of int
        quantized cdf of all input alphabet, len(c) = len(p)
        c[0] = 0, and c[-1] = sum(p[:-1])
    r : int
        bit-width precision of the quantized pmf
        sum(p) == 2 ** r

    Warnings
    --------
    int type for all the input arguments should be python int type,
    which has arbitrary precision

    Returns
    -------
    s : integer representation of the encoded message

    """</span>
    <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">symbols</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">s</span> <span class="o">&lt;</span> <span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">s</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span> <span class="o">*</span> <span class="p">(</span><span class="n">s</span> <span class="o">//</span> <span class="n">p</span><span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="o">+</span> \
            <span class="n">s</span> <span class="o">%</span> <span class="n">p</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">+</span> <span class="n">c</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">s</span>
</code></pre></div></div>

<h3 id="decoder-1">Decoder</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ans_decoder</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
    <span class="s">""" ANS encoder (no rescaling)

    Parameters
    ----------
    s : int
        integer representation of the encoded message
    p : list of int
        quantized pmf of all input alphabet, sum(p) == 2 ** r
    c : list of int
        quantized cdf of all input alphabet, len(c) = len(p)
        c[0] = 0, and c[-1] = sum(p[:-1])
    r : int
        bit-width precision of the quantized pmf
        sum(p) == 2 ** r

    Warnings
    --------
    int type for all the input arguments should be python int type,
    which has arbitrary precision

    Returns
    -------
    decoded_symbols : list of int
        list of decoded symbols

    """</span>

    <span class="k">def</span> <span class="nf">h</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">s</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span>
        <span class="c1"># this loop can be improved by binary search
</span>        <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">c</span><span class="p">))):</span>
            <span class="k">if</span> <span class="n">s</span> <span class="o">&gt;=</span> <span class="n">c</span><span class="p">[</span><span class="n">a</span><span class="p">]:</span>
                <span class="k">return</span> <span class="n">a</span>

    <span class="n">decoded_symbols</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">while</span> <span class="n">s</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">decoded_symbols</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">p</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">s</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span><span class="p">)</span> <span class="o">+</span> \
            <span class="n">s</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="n">r</span><span class="p">)</span> <span class="o">-</span> <span class="n">c</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">s</span> <span class="o">&lt;</span> <span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">s</span> <span class="o">-=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">decoded_symbols</span><span class="p">))</span>
</code></pre></div></div>
<h3 id="test">Test</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="c1"># initialize data distribution and input length
</span><span class="n">p</span> <span class="o">=</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">106</span><span class="p">]</span>
<span class="n">c</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">150</span><span class="p">]</span>
<span class="n">r</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">sequence_length</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># randomly sample input
</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">symbols</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">choices</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">)),</span> <span class="n">weights</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">sequence_length</span><span class="p">)</span>

<span class="c1"># encode
</span><span class="n">s</span> <span class="o">=</span> <span class="n">ans_encoder</span><span class="p">(</span><span class="n">symbols</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>

<span class="c1"># decode
</span><span class="n">decoded_symbols</span> <span class="o">=</span> <span class="n">ans_decoder</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>

<span class="c1"># statistics
</span><span class="n">average_bps</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="n">log2</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">/</span> <span class="n">sequence_length</span>
<span class="n">entropy</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="n">i</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span> <span class="o">*</span> <span class="n">math</span><span class="p">.</span><span class="n">log2</span><span class="p">(</span><span class="n">i</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">p</span><span class="p">)</span>

<span class="c1"># sanity check
</span><span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">decoded_symbols</span><span class="p">,</span> <span class="n">symbols</span><span class="p">))</span>

<span class="c1"># display results
</span><span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"encoded integer        : </span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"average bits per symbol: </span><span class="si">{</span><span class="n">average_bps</span><span class="p">:.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="s"> bits/symbol"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"data source entropy    : </span><span class="si">{</span><span class="n">entropy</span><span class="p">:.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="s"> bits/symbol"</span><span class="p">)</span>
</code></pre></div></div>

<p>Test output</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">encoded</span> <span class="n">integer</span>        <span class="p">:</span> <span class="mi">125621967822099623663819958660494947377946858741001513589</span>
<span class="n">average</span> <span class="n">bits</span> <span class="n">per</span> <span class="n">symbol</span><span class="p">:</span> <span class="mf">1.86357</span> <span class="n">bits</span><span class="o">/</span><span class="n">symbol</span>
<span class="n">data</span> <span class="n">source</span> <span class="n">entropy</span>    <span class="p">:</span> <span class="mf">1.79865</span> <span class="n">bits</span><span class="o">/</span><span class="n">symbol</span>
</code></pre></div></div>

<h2 id="ans-with-rescaling">ANS with rescaling</h2>

<p>The above ANS implementation takes advantage of the fact that python integer has arbitrary precision, which allows us to encode a sequence that is arbitrarily long without overflowing. This poses a complexity issue: the encoding operation gets increasingly hard to compute as integer \(s\) gets larger. Without resolving this reliance on infinite precision integer arithmetic, we cannot implement it using lower-level language with more hardware friendly instructions.</p>

<p>One idea is to limit the range of \(s\), say with a maximum bit-width of \(r_s\). Since the encoding process will necessary increase the value of \(s\), we then need to scale down its value before additional encoding, to a point where we can avoid overflow. In other words, before carrying out the encoding operation os \(e(s', x)\), \(s'\) should satisfy</p>

\[\begin{align*}
&amp;&amp;2^r\times (s'//p(x)) + (s'\text{ mod }p(x) + c(x)) &amp;&lt; 2^{r_s}\\
\Longleftrightarrow&amp;&amp; s'//p(x) + \underbrace{(s'\text{ mod }p(x) + c(x)) / 2^r}_{&lt;1}&amp;&lt; 2^{r_s-r}\\
\Longleftrightarrow&amp;&amp; s'//p(x) &amp;&lt; 2^{r_s-r}\\
\Longleftrightarrow&amp;&amp; s' &amp;&lt; (2^{r_s-r}+1)\times p(x)
\end{align*}\]

<p>In the rescaling implementation, scaling down is achieved by extracting \(r_t\) least significant bits, packing these bits into an integer \(t\), and saving this integer to a stack \(t_\text{stack}\), achieved through the following logic</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">while</span> <span class="n">s</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">r_s</span> <span class="o">-</span> <span class="n">r</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">p</span><span class="p">[</span><span class="n">x</span><span class="p">]:</span>
    <span class="n">t</span> <span class="o">=</span> <span class="p">(</span><span class="n">s</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r_t</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">&gt;&gt;=</span> <span class="n">r_t</span>
    <span class="n">t_stack</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</code></pre></div></div>

<p>Now we have guaranteed that encoder output will be an integer \(s&lt;2^{r_s}\) accompanied by a stack of integers \(t&lt;2^{r_t}\), the next question is, how to perform decoding? An easy answer would be to do the exact inverse of encoding, but to do that we need to know exactly when to perform up-scaling. The key is to realize that after the above loop is performed, it is guaranteed that \(e(s', x)\) is always larger than or equal to \(2^{r_s-r_t}\) (assuming \(r_t &gt; r\)), and thus during decoding we just need to upscale \(s\) whenever it falls below \(2^{r_s-r_t}\).</p>

<p>After the while loop terminates, we have</p>

\[\begin{align*}
&amp;&amp; 2^{r_t} s' + (2^{r_t}-1) &amp;\geq (2^{r_s-r}+1) p(x)\\
\Longleftrightarrow &amp;&amp; 2^{r_t}s' + 2^{r_t} &amp;&gt; (2^{r_s-r}+1)p(x)\\
\Longleftrightarrow &amp;&amp; 2^{r_t}s'&amp;&gt; 2^{r_s-r}p(x) + p(x) - 2^{r_t}\\
\Longleftrightarrow &amp;&amp;        s'&amp;&gt; 2^{r_s-r-r_t}p(x) + \underbrace{p(x)/2^{r_t}}_{&lt;1} - 1\\
\Longleftrightarrow &amp;&amp;        s'&amp;\geq 2^{r_s-r-r_t}p(x)
\end{align*}\]

<p>Plugging in the above inequality to \(e(s', x)\), we have</p>

\[\begin{align*}
e(s', x) \geq 2^{r_s -r_t}.
\end{align*}\]

<p>Now we are ready to implement ANS with rescaling.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ans_encoder</span><span class="p">(</span><span class="n">symbols</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_s</span><span class="p">,</span> <span class="n">r_t</span><span class="p">):</span>
    <span class="s">""" ANS encoder

    Parameters
    ----------
    symbols : list of int
        list of input symbols represented by index
        value should not be larger than len(p)
    p : list of int
        quantized pmf of all input alphabet, sum(p) == 2 ** r
    c : list of int
        quantized cdf of all input alphabet, len(c) = len(p)
        c[0] = 0, and c[-1] = sum(p[:-1])
    r : int
        bit-width precision of the quantized pmf
        sum(p) == 2 ** r
    r_s : int
        bit-width precision of encoded integer s
    r_t : int
        bit-width precision of integers in stack t_stack

    Returns
    -------
    s : int
        s &lt; 2 ** r_s
    t_stack : list of int
        each int &lt; 2 ** r_t

    """</span>
    <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">t_stack</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">symbols</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">s</span> <span class="o">&lt;</span> <span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">while</span> <span class="n">s</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">r_s</span> <span class="o">-</span> <span class="n">r</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">p</span><span class="p">[</span><span class="n">x</span><span class="p">]:</span>
            <span class="n">t</span> <span class="o">=</span> <span class="n">s</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="n">r_t</span><span class="p">)</span>
            <span class="n">s</span> <span class="o">&gt;&gt;=</span> <span class="n">r_t</span>
            <span class="n">t_stack</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span> <span class="o">*</span> <span class="p">(</span><span class="n">s</span> <span class="o">//</span> <span class="n">p</span><span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="o">+</span> \
            <span class="n">s</span> <span class="o">%</span> <span class="n">p</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">+</span> <span class="n">c</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">s</span><span class="p">,</span> <span class="n">t_stack</span>


<span class="k">def</span> <span class="nf">ans_decoder</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">t_stack</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_s</span><span class="p">,</span> <span class="n">r_t</span><span class="p">):</span>
    <span class="s">""" ANS encoder

    Parameters
    ----------
    s : int
        (s, t_stack) together represent the encoded message; s &lt; 2 ** r_s
    t_stack : list of int
        (s, t_stack) together represent the encoded message; t &lt; 2 ** r_t
    p : list of int
        quantized pmf of all input alphabet, sum(p) == 2 ** r
    c : list of int
        quantized cdf of all input alphabet, len(c) = len(p)
        c[0] = 0, and c[-1] = sum(p[:-1])
    r : int
        bit-width precision of the quantized pmf
        sum(p) == 2 ** r
    r_s : int
        bit-width precision of encoded integer s
    r_t : int
        bit-width precision of integers in stack t_stack

    Returns
    -------
    decoded_symbols : list of int
        list of decoded symbols

    """</span>

    <span class="k">def</span> <span class="nf">h</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">s</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span>
        <span class="c1"># this loop can be improved by binary search
</span>        <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">c</span><span class="p">))):</span>
            <span class="k">if</span> <span class="n">s</span> <span class="o">&gt;=</span> <span class="n">c</span><span class="p">[</span><span class="n">a</span><span class="p">]:</span>
                <span class="k">return</span> <span class="n">a</span>

    <span class="n">decoded_symbols</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">while</span> <span class="n">s</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">decoded_symbols</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">p</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">s</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span><span class="p">)</span> <span class="o">+</span> \
            <span class="n">s</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="n">r</span><span class="p">)</span> <span class="o">-</span> <span class="n">c</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">s</span> <span class="o">&lt;</span> <span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">s</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="k">while</span> <span class="n">s</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">r_s</span> <span class="o">-</span> <span class="n">r_t</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">t_stack</span><span class="p">):</span>
            <span class="n">t</span> <span class="o">=</span> <span class="n">t_stack</span><span class="p">.</span><span class="n">pop</span><span class="p">()</span>
            <span class="n">s</span> <span class="o">=</span> <span class="p">(</span><span class="n">s</span> <span class="o">&lt;&lt;</span> <span class="n">r_t</span><span class="p">)</span> <span class="o">+</span> <span class="n">t</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">decoded_symbols</span><span class="p">))</span>


<span class="c1">## Test code
</span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="c1"># initialize data distribution and input length
</span><span class="n">p</span> <span class="o">=</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">106</span><span class="p">]</span>
<span class="n">c</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">150</span><span class="p">]</span>
<span class="n">r</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">r_s</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">r_t</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">sequence_length</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="c1"># randomly sample input
</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">symbols</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">choices</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">)),</span> <span class="n">weights</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">sequence_length</span><span class="p">)</span>

<span class="c1"># encode
</span><span class="n">s</span><span class="p">,</span> <span class="n">t_stack</span> <span class="o">=</span> <span class="n">ans_encoder</span><span class="p">(</span><span class="n">symbols</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_s</span><span class="p">,</span> <span class="n">r_t</span><span class="p">)</span>

<span class="c1"># decode
</span><span class="n">decoded_symbols</span> <span class="o">=</span> <span class="n">ans_decoder</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">t_stack</span><span class="p">[:],</span> <span class="n">p</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_s</span><span class="p">,</span> <span class="n">r_t</span><span class="p">)</span>

<span class="c1"># statistics
</span><span class="n">average_bps</span> <span class="o">=</span> <span class="p">(</span><span class="n">r_s</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">t_stack</span><span class="p">)</span> <span class="o">*</span> <span class="n">r_t</span><span class="p">)</span> <span class="o">/</span> <span class="n">sequence_length</span>
<span class="n">entropy</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="n">i</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span> <span class="o">*</span> <span class="n">math</span><span class="p">.</span><span class="n">log2</span><span class="p">(</span><span class="n">i</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">r</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">p</span><span class="p">)</span>

<span class="c1"># sanity check
</span><span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">decoded_symbols</span><span class="p">,</span> <span class="n">symbols</span><span class="p">))</span>

<span class="c1"># display results
</span><span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"average bits per symbol: </span><span class="si">{</span><span class="n">average_bps</span><span class="p">:.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="s"> bits/symbol"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"data source entropy    : </span><span class="si">{</span><span class="n">entropy</span><span class="p">:.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="s"> bits/symbol"</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="test-results">Test results</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">average</span> <span class="n">bits</span> <span class="n">per</span> <span class="n">symbol</span><span class="p">:</span> <span class="mf">1.80960</span> <span class="n">bits</span><span class="o">/</span><span class="n">symbol</span>
<span class="n">data</span> <span class="n">source</span> <span class="n">entropy</span>    <span class="p">:</span> <span class="mf">1.79865</span> <span class="n">bits</span><span class="o">/</span><span class="n">symbol</span>
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[an introduction of ANS and its implementation]]></summary></entry><entry><title type="html">Arithmetic Coding (AC) Implementation</title><link href="https://yyang768osu.github.io/blog/2020/arithmetic-coding-implementation/" rel="alternate" type="text/html" title="Arithmetic Coding (AC) Implementation" /><published>2020-06-24T00:00:00+00:00</published><updated>2020-06-24T00:00:00+00:00</updated><id>https://yyang768osu.github.io/blog/2020/arithmetic-coding-implementation</id><content type="html" xml:base="https://yyang768osu.github.io/blog/2020/arithmetic-coding-implementation/"><![CDATA[<h2 id="arithmetic-encoder-infinite-precision">Arithmetic Encoder (infinite precision)</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="n">b</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">symbols</span><span class="p">:</span>
    <span class="n">width</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">a</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">width</span> <span class="o">*</span> <span class="n">c</span><span class="p">[</span><span class="n">symbol</span><span class="p">],</span> <span class="n">a</span> <span class="o">+</span> <span class="n">width</span> <span class="o">*</span> <span class="n">d</span><span class="p">[</span><span class="n">symbol</span><span class="p">]</span>
<span class="k">while</span> <span class="n">b</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">a</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">b</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">:</span>  <span class="c1"># case 0
</span>        <span class="n">bits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">*=</span> <span class="mi">2</span>
        <span class="n">b</span> <span class="o">*=</span> <span class="mi">2</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># case 1
</span>        <span class="n">bits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1"># a &lt; 1/2 and b &gt; 1/2
</span><span class="k">while</span> <span class="n">a</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="mi">3</span> <span class="o">/</span> <span class="mi">4</span><span class="p">:</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">a</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">s</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="c1"># a &lt;= 1/4 or b &gt;= 3/4
</span><span class="k">if</span> <span class="n">a</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">4</span><span class="p">:</span>  <span class="c1"># case 2a
</span>    <span class="n">bits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">bits</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>
<span class="k">else</span><span class="p">:</span>  <span class="c1"># case 2b
</span>    <span class="n">bits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">bits</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>
</code></pre></div></div>

<h2 id="arithmetic-decoder-infinite-precision">Arithmetic Decoder (infinite precision)</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">decoded_symbols</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">z</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">a</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">b</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="k">for</span> <span class="n">bit_index</span><span class="p">,</span> <span class="n">bit</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bits</span><span class="p">):</span>
    <span class="n">binary_block_size</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="n">bit_index</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bit</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">z</span> <span class="o">+=</span> <span class="n">binary_block_size</span>
    <span class="n">symbol</span> <span class="o">=</span> <span class="n">decode_one_symbol</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">z</span> <span class="o">+</span> <span class="n">binary_block_size</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="k">while</span> <span class="n">symbol</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">decoded_symbols</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">symbol</span><span class="p">)</span>
        <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">c</span><span class="p">[</span><span class="n">symbol</span><span class="p">],</span> <span class="n">a</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">d</span><span class="p">[</span><span class="n">symbol</span><span class="p">]</span>
        <span class="n">symbol</span> <span class="o">=</span> <span class="n">decode_one_symbol</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">z</span> <span class="o">+</span> <span class="n">binary_block_size</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">decode_one_symbol</span><span class="p">(</span><span class="n">z_0</span><span class="p">,</span> <span class="n">z_1</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="s">"""
    Parameters
    ----------
    z_0: lower end of the current binary block
    z_1: higher end of the current binary block
    a: lower end of the current sub-interval
    b: higher end of the current sub-interval
    c: CDF starts with a 0.0
    d: CDF that ends with 1.0

    Returns
    -------
    if [z_0, z_1] is not contained in any of the symbols inside [a, b]:
        return None
    else:
        return the decoded index

    """</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)):</span>
        <span class="n">low</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">low</span>
        <span class="n">high</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">high</span>
        <span class="k">if</span> <span class="n">low</span> <span class="o">&lt;=</span> <span class="n">z_0</span> <span class="ow">and</span> <span class="n">z_1</span> <span class="o">&lt;=</span> <span class="n">high</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">index</span>

</code></pre></div></div>

<h2 id="arithmetic-encoder-with-rescaling-infinite-precision">Arithmetic Encoder with Rescaling (infinite precision)</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="n">b</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">symbols</span><span class="p">:</span>
    <span class="n">width</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">a</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">width</span> <span class="o">*</span> <span class="n">c</span><span class="p">[</span><span class="n">symbol</span><span class="p">],</span> <span class="n">a</span> <span class="o">+</span> <span class="n">width</span> <span class="o">*</span> <span class="n">d</span><span class="p">[</span><span class="n">symbol</span><span class="p">]</span>
    <span class="k">while</span> <span class="n">b</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">a</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">b</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">:</span>  <span class="c1"># case 0
</span>            <span class="n">bits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">bits</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>
            <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">a</span> <span class="o">*=</span> <span class="mi">2</span>
            <span class="n">b</span> <span class="o">*=</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># case 1
</span>            <span class="n">bits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">bits</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>
            <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">a</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="c1"># a &lt; 1/2 and b &gt; 1/2
</span>    <span class="k">while</span> <span class="n">a</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="mi">3</span> <span class="o">/</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">a</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">4</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">s</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="c1"># a &lt;= 1/4 or b &gt;= 3/4
</span><span class="k">if</span> <span class="n">a</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">4</span><span class="p">:</span>  <span class="c1"># case 2a
</span>    <span class="n">bits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">bits</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>
<span class="k">else</span><span class="p">:</span>  <span class="c1"># case 2b
</span>    <span class="n">bits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">bits</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>
</code></pre></div></div>

<h2 id="arithmetic-encoder-with-rescaling-finite-precision">Arithmetic Encoder with Rescaling (finite precision)</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">range_precision</span>
<span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">symbols</span><span class="p">:</span>
    <span class="n">width</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">a</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">width</span> <span class="o">*</span> <span class="n">c</span><span class="p">[</span><span class="n">symbol</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">pmf_precision</span><span class="p">,</span> <span class="n">a</span> <span class="o">+</span> <span class="n">width</span> <span class="o">*</span> <span class="n">d</span><span class="p">[</span><span class="n">symbol</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">pmf_precision</span>
    <span class="k">while</span> <span class="n">b</span> <span class="o">&lt;=</span> <span class="n">range_half</span> <span class="ow">or</span> <span class="n">a</span> <span class="o">&gt;=</span> <span class="n">range_half</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">b</span> <span class="o">&lt;=</span> <span class="n">range_half</span><span class="p">:</span>  <span class="c1"># case 0
</span>            <span class="n">bits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">bits</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>
            <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">a</span> <span class="o">*=</span> <span class="mi">2</span>
            <span class="n">b</span> <span class="o">*=</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># case 1
</span>            <span class="n">bits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">bits</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>
            <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">a</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">range_half</span><span class="p">)</span>
            <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">range_half</span><span class="p">)</span>
    <span class="c1"># a &lt; 1/2 and b &gt; 1/2
</span>    <span class="k">while</span> <span class="n">a</span> <span class="o">&gt;</span> <span class="n">range_quarter</span> <span class="ow">and</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">range_quarter</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">a</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">range_quarter</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">range_quarter</span><span class="p">)</span>
<span class="n">s</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="c1"># a &lt;= 1/4 or b &gt;= 3/4
</span><span class="k">if</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="n">range_quarter</span><span class="p">:</span>  <span class="c1"># case 2a
</span>    <span class="n">bits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">bits</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>
<span class="k">else</span><span class="p">:</span>  <span class="c1"># case 2b
</span>    <span class="n">bits</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">bits</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">s</span>
</code></pre></div></div>

<h2 id="arithmetic-decoder-with-rescaling-finite-precision">Arithmetic Decoder with Rescaling (finite precision)</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">z</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">range_precision</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="p">(</span><span class="n">z</span> <span class="o">&lt;&lt;</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">bits</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">+=</span> <span class="n">bits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">next_bit_index</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">bits</span><span class="p">),</span> <span class="n">range_precision</span><span class="p">)</span>
<span class="n">z_gap</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">range_precision</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">bits</span><span class="p">))</span>

<span class="n">decoded_symbols</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">a</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">range_precision</span>
<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">)):</span>
        <span class="n">low</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">low</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">pmf_precision</span>
        <span class="n">high</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">high</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">**</span> <span class="n">pmf_precision</span>
        <span class="k">if</span> <span class="n">low</span> <span class="o">&lt;=</span> <span class="n">z</span> <span class="ow">and</span> <span class="n">high</span> <span class="o">&gt;=</span> <span class="n">z</span> <span class="o">+</span> <span class="n">z_gap</span><span class="p">:</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">low</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">high</span>
            <span class="n">decoded_symbols</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
            <span class="k">break</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="k">while</span> <span class="n">b</span> <span class="o">&lt;=</span> <span class="n">range_half</span> <span class="ow">or</span> <span class="n">a</span> <span class="o">&gt;=</span> <span class="n">range_half</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">b</span> <span class="o">&lt;=</span> <span class="n">range_half</span><span class="p">:</span>
            <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">b</span>
            <span class="n">a</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">a</span>
            <span class="n">z</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">z</span>
            <span class="k">if</span> <span class="n">next_bit_index</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">bits</span><span class="p">):</span>
                <span class="n">z</span> <span class="o">+=</span> <span class="n">bits</span><span class="p">[</span><span class="n">next_bit_index</span><span class="p">]</span>
                <span class="n">next_bit_index</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">z_gap</span> <span class="o">&lt;&lt;=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">range_half</span><span class="p">)</span>
            <span class="n">a</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">range_half</span><span class="p">)</span>
            <span class="n">z</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">z</span> <span class="o">-</span> <span class="n">range_half</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">next_bit_index</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">bits</span><span class="p">):</span>
                <span class="n">z</span> <span class="o">+=</span> <span class="n">bits</span><span class="p">[</span><span class="n">next_bit_index</span><span class="p">]</span>
                <span class="n">next_bit_index</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">z_gap</span> <span class="o">&lt;&lt;=</span> <span class="mi">1</span>
    <span class="k">while</span> <span class="n">a</span> <span class="o">&gt;</span> <span class="n">range_quarter</span> <span class="ow">and</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">range_quarter</span><span class="p">:</span>
        <span class="n">a</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">range_quarter</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">range_quarter</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">z</span> <span class="o">-</span> <span class="n">range_quarter</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">next_bit_index</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">bits</span><span class="p">):</span>
            <span class="n">z</span> <span class="o">+=</span> <span class="n">bits</span><span class="p">[</span><span class="n">next_bit_index</span><span class="p">]</span>
            <span class="n">next_bit_index</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">z_gap</span> <span class="o">&lt;&lt;=</span> <span class="mi">1</span>
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[python implementation of arithmetic coding]]></summary></entry><entry><title type="html">Optical Flow – An Overview</title><link href="https://yyang768osu.github.io/blog/2020/optical-flow-an-overview/" rel="alternate" type="text/html" title="Optical Flow – An Overview" /><published>2020-06-16T00:00:00+00:00</published><updated>2020-06-16T00:00:00+00:00</updated><id>https://yyang768osu.github.io/blog/2020/optical-flow-an-overview</id><content type="html" xml:base="https://yyang768osu.github.io/blog/2020/optical-flow-an-overview/"><![CDATA[<ul id="markdown-toc">
  <li><a href="#definition-of-optical-flow" id="markdown-toc-definition-of-optical-flow">Definition of Optical Flow</a></li>
  <li><a href="#useful-resources" id="markdown-toc-useful-resources">Useful Resources</a></li>
  <li><a href="#traditional-approach" id="markdown-toc-traditional-approach">Traditional Approach</a>    <ul>
      <li><a href="#brightness-constancy-assumption" id="markdown-toc-brightness-constancy-assumption">Brightness Constancy Assumption</a></li>
      <li><a href="#small-motion-assumption" id="markdown-toc-small-motion-assumption">Small Motion Assumption</a></li>
      <li><a href="#brightness-constancy-equation" id="markdown-toc-brightness-constancy-equation">Brightness Constancy Equation</a></li>
      <li><a href="#how-to-solve-brightness-constancy-equation" id="markdown-toc-how-to-solve-brightness-constancy-equation">How to solve Brightness Constancy Equation?</a></li>
      <li><a href="#formulation-of-horn-shunck-optical-flow" id="markdown-toc-formulation-of-horn-shunck-optical-flow">Formulation of Horn-Shunck Optical flow</a></li>
      <li><a href="#discrete-optical-flow-estimation" id="markdown-toc-discrete-optical-flow-estimation">Discrete Optical Flow Estimation</a></li>
    </ul>
  </li>
  <li><a href="#dataset" id="markdown-toc-dataset">Dataset</a>    <ul>
      <li><a href="#middlebury-link-paper" id="markdown-toc-middlebury-link-paper">Middlebury (link, paper)</a></li>
      <li><a href="#mpi-sintel-link-paper" id="markdown-toc-mpi-sintel-link-paper">MPI Sintel (link, paper)</a></li>
      <li><a href="#kitti-link-paper" id="markdown-toc-kitti-link-paper">KITTI (link, paper)</a></li>
      <li><a href="#flying-chairs-link-paper" id="markdown-toc-flying-chairs-link-paper">Flying Chairs (link, paper)</a></li>
      <li><a href="#flying-things-3d-link-paper" id="markdown-toc-flying-things-3d-link-paper">Flying Things 3D (link, paper)</a></li>
    </ul>
  </li>
  <li><a href="#evaluation-metric" id="markdown-toc-evaluation-metric">Evaluation Metric</a>    <ul>
      <li><a href="#angular-error-ae" id="markdown-toc-angular-error-ae">Angular Error (AE)</a></li>
      <li><a href="#end-point-error-epe" id="markdown-toc-end-point-error-epe">End Point Error (EPE)</a></li>
    </ul>
  </li>
  <li><a href="#end-to-end-regression-based-optical-flow-estimation" id="markdown-toc-end-to-end-regression-based-optical-flow-estimation">End-to-end regression based optical flow estimation</a>    <ul>
      <li><a href="#some-useful-concepts" id="markdown-toc-some-useful-concepts">Some useful concepts</a></li>
      <li><a href="#overview-of-different-models" id="markdown-toc-overview-of-different-models">Overview of different models</a></li>
      <li><a href="#flownet-iccv-2015-paper" id="markdown-toc-flownet-iccv-2015-paper">FlowNet (ICCV 2015) paper</a></li>
      <li><a href="#flownet-20-cvpr-2017-paper" id="markdown-toc-flownet-20-cvpr-2017-paper">FlowNet 2.0 (CVPR 2017) paper</a></li>
      <li><a href="#spynet-cvpr-2017-paper-code" id="markdown-toc-spynet-cvpr-2017-paper-code">SPyNet (CVPR 2017) paper code</a></li>
      <li><a href="#pwcnet-cvpr-2018-paper-code-video" id="markdown-toc-pwcnet-cvpr-2018-paper-code-video">PWCNet (CVPR 2018) paper code video</a></li>
      <li><a href="#irr-pwcnet-cvpr-2019-paper" id="markdown-toc-irr-pwcnet-cvpr-2019-paper">IRR-PWCNet (CVPR 2019) paper</a></li>
      <li><a href="#pwcnet-fusion-wacv-2019-paper" id="markdown-toc-pwcnet-fusion-wacv-2019-paper">PWCNet Fusion (WACV 2019) paper</a></li>
      <li><a href="#scopeflow-cvpr-2020-paper-code" id="markdown-toc-scopeflow-cvpr-2020-paper-code">ScopeFlow (CVPR 2020) paper code</a></li>
      <li><a href="#maskflownet-cvpr-2020-paper-code" id="markdown-toc-maskflownet-cvpr-2020-paper-code">MaskFlownet (CVPR 2020) paper code</a></li>
      <li><a href="#raft-recurrent-all-pairs-field-transforms-for-optical-flow-arxiv-2020-paper-code" id="markdown-toc-raft-recurrent-all-pairs-field-transforms-for-optical-flow-arxiv-2020-paper-code">RAFT: REcurrent All-Pairs Field Transforms for Optical Flow (Arxiv 2020) paper code</a></li>
    </ul>
  </li>
</ul>

<h2 id="definition-of-optical-flow">Definition of Optical Flow</h2>

<p>Distribution of apparent velocities of movement of brightness pattern in an image.</p>

<ul>
  <li>Where do we need it?
    <ul>
      <li>Action recognition</li>
      <li>Motion segmentation</li>
      <li>Video compression</li>
    </ul>
  </li>
</ul>

<h2 id="useful-resources">Useful Resources</h2>

<ul>
  <li>CMU Computer Vision 16-385
    <ul>
      <li><a href="http://www.cs.cmu.edu/~16385/s17/Slides/14.1_Brightness_Constancy.pdf">Brightness Constancy</a></li>
      <li><a href="http://www.cs.cmu.edu/~16385/s17/Slides/14.2_OF__ConstantFlow.pdf">Optical Flow : Constant Flow</a></li>
      <li><a href="http://www.cs.cmu.edu/~16385/s15/lectures/Lecture21.pdf">Optical Flow : Lucas-Kanade</a></li>
      <li><a href="http://www.cs.cmu.edu/~16385/s17/Slides/14.3_OF__HornSchunck.pdf">Optical Flow : Horn-Shunck</a></li>
    </ul>
  </li>
  <li>CMU Computer Vision 16-720
    <ul>
      <li><a href="http://16720.courses.cs.cmu.edu/lec/motion_lec12.pdf">Motion and Flow</a></li>
      <li><a href="http://16720.courses.cs.cmu.edu/lec/flow.pdf">Estimating Optical Flow 1</a></li>
      <li><a href="http://16720.courses.cs.cmu.edu/lec/flow_lec13.pdf">Estimating Optical Flow 2</a></li>
    </ul>
  </li>
  <li>Papers
    <ul>
      <li><a href="https://arxiv.org/abs/1504.06852">FlowNet: Learning Optical Flow with Convolutional Networks (ICCV 2015)</a></li>
      <li><a href="https://arxiv.org/abs/1612.01925">FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks (CVPR 2017)</a></li>
      <li><a href="https://arxiv.org/abs/1611.00850">Optical Flow Estimation using a Spatial Pyramid Network (CVPR 2017)</a></li>
      <li><a href="https://arxiv.org/abs/2004.02853">Optical Flow Estimation in the Deep Learning Age (2020/04/06)</a></li>
      <li><a href="https://arxiv.org/abs/1709.02371">PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume (CVPR 2018)</a></li>
      <li><a href="https://arxiv.org/abs/1904.05290">Iterative Residual Refinement for Joint Optical Flow and Occlusion Estimation (CVPR 2019)</a></li>
      <li><a href="https://arxiv.org/abs/1810.10066">A fusion approach for multi-frame optical flow estimation (WACV 2019)</a></li>
      <li><a href="https://arxiv.org/abs/2002.10770">ScopeFlow: Dynamic Scene Scoping for Optical Flow (CVPR 2020)</a></li>
      <li><a href="https://arxiv.org/abs/2003.10955">MaskFlownet: Asymmetric Feature Matching with Learnable Occlusion Mask (CVPR 2020)</a></li>
      <li><a href="https://arxiv.org/abs/2003.12039">RAFT: REcurrent All-Pairs Field Transforms for Optical Flow (Arxiv 2020)</a></li>
    </ul>
  </li>
</ul>

<h2 id="traditional-approach">Traditional Approach</h2>

<h3 id="brightness-constancy-assumption">Brightness Constancy Assumption</h3>

\[\begin{align*}
I(x(t), y(t), t) = C
\end{align*}\]

<h3 id="small-motion-assumption">Small Motion Assumption</h3>

\[\begin{align*}
&amp;I(x(t+\delta t), y(t+\delta t), t+ \delta t) = I(x(t), y(t), t) + \frac{dI}{dt}\delta t, \text{(higher order term ignored)}\\
&amp;\text{where }\frac{dI}{dt} = \frac{\partial I}{\partial x}\frac{d x}{d t} + \frac{\partial I}{\partial y}\frac{d y}{d t} + \frac{\partial I}{\partial t}\triangleq I_x u + I_y v + I_t \triangleq \nabla^T I [u, v]^T + I_t
\end{align*}\]

<p>\(\nabla I = [I_x, I_y]^T\) : spatial derivative</p>

<p>\(I_t\) : temporal derivative</p>

<p>\([u, v]\) : optical flow velocities</p>

<h3 id="brightness-constancy-equation">Brightness Constancy Equation</h3>
<p>Combining the above two assumptions, we obtain</p>

\[\begin{align*}
\nabla I [u, v]^T + I_t = 0.
\end{align*}\]

<h3 id="how-to-solve-brightness-constancy-equation">How to solve Brightness Constancy Equation?</h3>
<p>Temporal derivative \(I_t\) can be estimated by frame difference; spatial derivative \(\nabla I\) can be estimated using spatial filters. Since there are two unknowns (\(u\) and \(v\)), the system is under-determined.</p>

<p>Two ways to enforce additional constraints:</p>

<ul>
  <li>Lucas-Kanade Optical Flow (1981) : assuming local patch has constant flow
    <ul>
      <li>LS can be applied to solve this overdetermined set of equations</li>
      <li>If there is lack of spatial gradient in a local path, then the set of equations could still be under-determined. This is referred to as the <code class="language-plaintext highlighter-rouge">aperture</code> problem</li>
      <li>If applied to only tractable patches, these are called sparse flow</li>
    </ul>
  </li>
  <li>Horn-Schunck Optical Flow (1981) : assuming a smooth flow field</li>
</ul>

<h3 id="formulation-of-horn-shunck-optical-flow">Formulation of Horn-Shunck Optical flow</h3>

<p>Brightness constancy constraint/loss :</p>

\[\begin{align*}
E_d(i, j) = \left[I_x(i,j) u(i,j)+I_y(i,j)v(i,j) + I_t(i,j)\right]^2
\end{align*}\]

<p>Smoothness constraint/loss :</p>

\[\begin{align*}
E_s(i, j) = \frac{1}{4}\left[(u(i,j)-u(i+1,j))^2, (u(i,j)-u(i,j+1))^2, (v(i,j)-v(i+1,j)^2, (v(i,j)-v(i,j+1))^2\right]
\end{align*}\]

<p>Solving for optical flow :</p>

\[\begin{align*}
\text{min}_{\bf{u}, \bf{v}} \sum_{i,j} E_d(i,j) + \lambda E_s(i,j)
\end{align*}\]

<p>Gradient descent can be used to solve the above optimization problem.</p>

<h3 id="discrete-optical-flow-estimation">Discrete Optical Flow Estimation</h3>

<p>Brightness Constancy Equation assumes small motion, which is in general not the case. If the movement is beyond 1 pixel, then higher order terms in the Taylor expansion of \(I(x(t), y(t), t)\) could dominate. There are two solutions</p>
<ol>
  <li>To reduce the resolution using coarse-to-fine architecture</li>
  <li>Resort to discrete optical flow estimation</li>
</ol>

<p>For case-2, we obtain optical flow estimate by minimizing the following objective</p>

\[\begin{align*}
&amp;E({\bf{z}}) = \sum_{i\in\mathcal{I}}D(z_i) + \sum_{(i,j)\in \mathcal{N}}S(z_i, z_j)\\
&amp;\text{where } z_i\triangleq (u_i, v_j), \mathcal{I}\triangleq\text{set of all pixels }, \mathcal{N}\triangleq\text{set of all neighboring pixels}
\end{align*}\]

<p>The above can be viewed as energy minimization in a Markov random field.</p>

<h2 id="dataset">Dataset</h2>

<p>Table 1 from <a href="https://arxiv.org/pdf/1504.06852.pdf">FlowNet</a></p>

<table>
  <thead>
    <tr>
      <th>Entry</th>
      <th>Frame Pairs</th>
      <th>Frames with ground truth</th>
      <th>Ground-truth density per frame</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Middlebury</td>
      <td>72</td>
      <td>72</td>
      <td>100%</td>
    </tr>
    <tr>
      <td>KITTI2012</td>
      <td>194</td>
      <td>194</td>
      <td>50%</td>
    </tr>
    <tr>
      <td>MPI Sintel</td>
      <td>1041</td>
      <td>1041</td>
      <td>100%</td>
    </tr>
    <tr>
      <td>Flying Chairs</td>
      <td>22872</td>
      <td>22972</td>
      <td>100%</td>
    </tr>
    <tr>
      <td>Flying Things 3D</td>
      <td>22872</td>
      <td>-</td>
      <td>100%</td>
    </tr>
  </tbody>
</table>

<h3 id="middlebury-link-paper">Middlebury (<a href="http://vision.middlebury.edu/flow/">link</a>, <a href="http://vision.middlebury.edu/flow/floweval-ijcv2011.pdf">paper</a>)</h3>
<p>Contains only 8 image pairs for training, with ground truth flows generated using four different techniques. Displacements are very small, typically below 10 pixels. (Section 4.1 in <a href="https://arxiv.org/pdf/1504.06852.pdf">FlowNet</a>)</p>

<h3 id="mpi-sintel-link-paper">MPI Sintel (<a href="http://sintel.is.tue.mpg.de">link</a>, <a href="http://files.is.tue.mpg.de/black/papers/ButlerECCV2012-corrected.pdf">paper</a>)</h3>

<p>Computer-animated action movie. There are three render passes with varying degree of realism</p>
<ul>
  <li>Albedo render pass</li>
  <li>Clean pass (adds natural shading, cast shadows, specular reflections, and more complex lighting effects)</li>
  <li>Final pass (adds motion blur,  focus blur, and atmospherical effect)</li>
</ul>

<p>Contains 1064 training / 564 withheld test flow fields</p>

<h3 id="kitti-link-paper">KITTI (<a href="http://www.cvlibs.net/datasets/kitti/">link</a>, <a href="http://ww.cvlibs.net/publications/Geiger2013IJRR.pdf">paper</a>)</h3>

<p>Contains 194 training image pairs and includes large displacements, but contains only a very special motion type. The ground truth is obtained from real world scenes by simultaneously recording the scenes with a camera and a 3D laser scanner. This assumes that the scene is rigid and that the motion stems from a moving observer. Moreover, motion of distant objects, such as the sky, cannot be captured, resulting in sparse optical flow ground truth.</p>

<h3 id="flying-chairs-link-paper">Flying Chairs (<a href="https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html">link</a>, <a href="https://arxiv.org/abs/1504.06852">paper</a>)</h3>

<p>Contains about 22k image pairs of chairs superimposed on random background images from Flickr. Random affine transformations are applied to chairs and background to obtain the second image and ground truth flow fields. The dataset contains only planar motions. (Section 3 in <a href="https://arxiv.org/abs/1612.01925">FlowNet 2.0</a>)</p>

<h3 id="flying-things-3d-link-paper">Flying Things 3D (<a href="https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html">link</a>, <a href="https://arxiv.org/pdf/1512.02134.pdf">paper</a>)</h3>

<p>A natural extension of the FlyingChairs dataset, having 22,872 larger 3D scenes with more complex motion patterns.</p>

<h2 id="evaluation-metric">Evaluation Metric</h2>

<h3 id="angular-error-ae">Angular Error (AE)</h3>
<p>AE between \((u_0, v_0)\) and \((u_1, v_1)\) is the angle in 3D space between \((u_0, v_0, 1.0)\) and \((u_1, v_1, 1.0)\). Error in large flow is penalized less than errors in small flow. (Section 4.1 in <a href="http://vision.middlebury.edu/flow/flowEval-iccv07.pdf">link</a>)</p>

<h3 id="end-point-error-epe">End Point Error (EPE)</h3>
<p>EPE between \((u_0, v_0)\) and \((u_1, v_1)\) is \(\sqrt{(u_0-u_1)^2 + (v_0-v_1)^2}\) (Euclidean distance).</p>

<p>For Sintel MPI, papers also often reports detailed breakdown of EPE for pixels with different distance to motion boundaries (\(d_{0-10}\), \(d_{10-60}\), \(d_{60-140}\)) and different velocities (\(s_{0-10}\), \(s_{10-40}\), \(s_{40+}\)).</p>

<h2 id="end-to-end-regression-based-optical-flow-estimation">End-to-end regression based optical flow estimation</h2>

<h3 id="some-useful-concepts">Some useful concepts</h3>

<ul>
  <li>Backward warping</li>
</ul>

<p>\(I_1(\cdot, \cdot)\triangleq I(\cdot, \cdot, t=1), I_2(\cdot, \cdot)\triangleq I(\cdot, \cdot, t=2)\). Optical flow field \(u, v\) satisfies \(I_1(x, y) = I_2(x+u, y+v)\). In other words, \(u,v\) tells us where each pixel in \(I_1\) is coming from, compared with \(I_2\), and given \(u, v\), we know how to move around (warp) the pixels in \(I_2\) to obtain \(I_1\). Here \(I_1\) is often referred to as the source image and \(I_2\) the target image – flow vector is defined per source image. Specifically, we can define a <code class="language-plaintext highlighter-rouge">warp</code> operation as below</p>

\[\begin{align*}
&amp;I_{\text{source}} = \texttt{warp}(I_\text{target}, f) \text{ where}\\
&amp;I_{\text{source}}(x, y) = I_\text{target}(x+u, y+v)
\end{align*}\]

<ul>
  <li>Compositivity of backward warping</li>
</ul>

\[\begin{align*}
\texttt{warp}(I_\text{target}, f_a+f_b) = \texttt{warp}\left(\texttt{warp}(I_\text{target}, f_a), f_b\right)
\end{align*}\]

<h3 id="overview-of-different-models">Overview of different models</h3>

<table>
  <thead>
    <tr>
      <th>Model Name</th>
      <th>Num of parameters</th>
      <th>inference speed</th>
      <th>Training time</th>
      <th>MPI Sintel final test EPE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>FlowNetS</td>
      <td>32M</td>
      <td>87.72fps</td>
      <td>4days</td>
      <td>7.218</td>
    </tr>
    <tr>
      <td>FlowNetC</td>
      <td>32M</td>
      <td>46.10fps</td>
      <td>6days</td>
      <td>7.883</td>
    </tr>
    <tr>
      <td>FlowNet2.0</td>
      <td>162M</td>
      <td>11.79fps</td>
      <td>14days</td>
      <td>6.016</td>
    </tr>
    <tr>
      <td>SPyNet</td>
      <td>1.2M</td>
      <td>-</td>
      <td>-</td>
      <td>8.360</td>
    </tr>
    <tr>
      <td>PWCNet</td>
      <td>8.7M</td>
      <td>35.01fps</td>
      <td>4.8days</td>
      <td>5.042</td>
    </tr>
  </tbody>
</table>

<p class="notice">The EPE column is taken from Table 2 of <a href="https://arxiv.org/abs/2004.02853">an overview paper</a>. The inference speed (on Pascal Titan X) and training time column is taken from Table 7 of <a href="https://arxiv.org/abs/1709.02371">PWCNet paper</a>.</p>

<h3 id="flownet-iccv-2015-paper">FlowNet (ICCV 2015) <a href="https://arxiv.org/abs/1504.06852">paper</a></h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/optical_flow/FlowNet_encoder-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/optical_flow/FlowNet_encoder-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/optical_flow/FlowNet_encoder-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/optical_flow/FlowNet_encoder.png" data-zoomable="" />

  </picture>

</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/optical_flow/FlowNet_decoder-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/optical_flow/FlowNet_decoder-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/optical_flow/FlowNet_decoder-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/optical_flow/FlowNet_decoder.png" data-zoomable="" />

  </picture>

</figure>

    </div>
</div>

<p>The first end-to-end CNN architecture for estimating optical flow. Two variants:</p>
<ul>
  <li>FlowNetS
    <ul>
      <li>A pair of input images is simply concatenated and then input to the U-shaped network that directly outputs optical flow.</li>
    </ul>
  </li>
  <li>FlowNetC
    <ul>
      <li>FlowNetC has a shared encoder for both images, which extracts a feature map for each input image, and a cost volume is constructed by measuring patch-level similarity between the two feature maps with a correlation operation. The result is fed into the subsequent network layers.</li>
    </ul>
  </li>
</ul>

<p>Multi-scale training loss is applied. Both models still under-perform energy-based approaches.</p>

<h3 id="flownet-20-cvpr-2017-paper">FlowNet 2.0 (CVPR 2017) <a href="https://arxiv.org/abs/1612.01925">paper</a></h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/optical_flow/FlowNet_2-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/optical_flow/FlowNet_2-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/optical_flow/FlowNet_2-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/optical_flow/FlowNet_2.png" data-zoomable="" />

  </picture>

</figure>

    </div>
</div>

<p>Key ideas:</p>
<ol>
  <li>By stacking multiple FlowNet style networks, one can sequentially refine the output from previous network modules.</li>
  <li>It is helpful to pre-train networks on a less challenging synthetic dataset first and then further train on a more challenging synthetic dataset with 3D motion and photometric effects</li>
</ol>

<p>End-to-end based approach starts to outperform energy-based ones.</p>

<h3 id="spynet-cvpr-2017-paper-code">SPyNet (CVPR 2017) <a href="https://arxiv.org/abs/1611.00850">paper</a> <a href="https://github.com/anuragranj/spynet">code</a></h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/optical_flow/SPyNet-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/optical_flow/SPyNet-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/optical_flow/SPyNet-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/optical_flow/SPyNet.png" data-zoomable="" />

  </picture>

</figure>

    </div>
</div>

<p>Key idea:</p>
<ul>
  <li>Incorporate classic <code class="language-plaintext highlighter-rouge">coarse-to-fine</code> concepts into CNN network and update residual flow over mulitple pyramid levels (5 image pyramid levels are used). Networks at different levels have separate parameters.</li>
</ul>

<p>Achieves comparable performance to FlowNet with 96% less number of parameters.</p>

<h3 id="pwcnet-cvpr-2018-paper-code-video">PWCNet (CVPR 2018) <a href="https://arxiv.org/abs/1709.02371">paper</a> <a href="https://github.com/NVlabs/PWC-Net">code</a> <a href="https://www.youtube.com/watch?v=vVU8XV0Ac_0">video</a></h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/optical_flow/PWCNet-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/optical_flow/PWCNet-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/optical_flow/PWCNet-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/optical_flow/PWCNet.png" data-zoomable="" />

  </picture>

</figure>

    </div>
</div>

<p>Key ideas:</p>
<ol>
  <li>Learned feature pyramid instead of image pyramid</li>
  <li>Warping of feature maps</li>
  <li>Computing a cost volume of learned feature maps (correlation)</li>
</ol>

<p>Computation steps:</p>
<ol>
  <li>Feature pyramid extractor: conv-net with down-sampling</li>
  <li>Target feature map is warped by up-sampled previous flow estimation</li>
  <li>Cost volume is computed based on source feature map and warped target feature map</li>
  <li>Optical flow estimator: a DenseNet type of network that takes (1) source feature map (2) cost volume (3) up-sampled previous optical flow estimate</li>
  <li>Context network: a dilated convolution network to post process the estimated optical flow</li>
</ol>

<p>Remarks:</p>
<ul>
  <li>Multi-scale training loss</li>
  <li>Network at each scale estimates the optical flow for that scale, not the residual optical flow (the addition happens implicitly inside the optical flow estimator).</li>
</ul>

<h3 id="irr-pwcnet-cvpr-2019-paper">IRR-PWCNet (CVPR 2019) <a href="https://arxiv.org/abs/1904.05290">paper</a></h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/optical_flow/IRR_PWCNet-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/optical_flow/IRR_PWCNet-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/optical_flow/IRR_PWCNet-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/optical_flow/IRR_PWCNet.png" data-zoomable="" />

  </picture>

</figure>

    </div>
</div>

<p>Key ideas</p>
<ul>
  <li>Take the output from a previous pass through the network as input and iteratively refine it by only using a single network block with shared weights, which allows the network to residually refine the previous estimate.</li>
  <li>For PWCNet, the decoder module at different pyramid level is achieved using a 1x1 convolution before feeding the source feature map to the optical flow estimator/decoder.</li>
  <li>Joint occlusion and bidirectional optical flow estimation leads to further performance enhancement.</li>
</ul>

<h3 id="pwcnet-fusion-wacv-2019-paper">PWCNet Fusion (WACV 2019) <a href="https://arxiv.org/abs/1810.10066">paper</a></h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/optical_flow/PWCNet_Fusion-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/optical_flow/PWCNet_Fusion-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/optical_flow/PWCNet_Fusion-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/optical_flow/PWCNet_Fusion.png" data-zoomable="" />

  </picture>

</figure>

    </div>
</div>

<p>The paper focuses on three-frame optical flow estimation problem: given \(I_{t-1}\), \(I_{t}\), and \(I_{t+1}\), estimate \(f_{t\to t+1}\).</p>

<p>Key ideas:</p>
<ul>
  <li>If we are given \(f_{t-1\to t}\) and \(f_{t\to t-1}\), and assume constant velocity of movement, then an estimate of \(f_{t\to t+1}\) can be formed by backward warping \(f_{t-1\to t}\) with \(f_{t\to t-1}\).
\(\begin{align*}
&amp;\widehat{f}_{t\to t+1} \triangleq \texttt{warp}(f_{t-1 \to t}, f_{t\to t-1}), \\
&amp;\widehat{f}_{t\to t+1}(x, y) \triangleq f_{t-1 \to t}\left(x+f_{t\to t-1}(x,y)(x), y+f_{t\to t-1}(x,y)(y)\right)
\end{align*}\)</li>
  <li>With three frames available, we can plug-in any two-frame optical flow estimation solution (PWCNet in this case) to obtain \(f_{t-1 \to t}\), \(f_{t\to t+1}\) and \(f_{t \to t-1}\).</li>
  <li>A fusion network (similar to the one used in the last stage of FlowNet 2.0) can be used to fuse together \(\widehat{f}_{t \to t-1}\triangleq\texttt{warp}(f_{t-1 \to t}, f_{t\to t-1})\) and \(f_{t \to t+1}\).
    <ul>
      <li>Note that \(\widehat{f}_{t\to t-1}\) would be identical to \(f_{t\to t+1}\) if (a) velocity is constant (b) three optical flow estimations are correct, and (c) there are no occlusions. Brightness constancy errors of the two flow maps together with the source frame \(I_t\) are fed into the fusion network to provide additional info.</li>
    </ul>
  </li>
</ul>

<p>Why multi-frame may perform better than 2-frame solutions:</p>
<ul>
  <li>temporal smoothness leads to additional regularization.</li>
  <li>longer time sequences may help in ambiguous situations such as occluded regions.</li>
</ul>

<h3 id="scopeflow-cvpr-2020-paper-code">ScopeFlow (CVPR 2020) <a href="https://arxiv.org/abs/2002.10770">paper</a> <a href="https://github.com/avirambh/ScopeFlow">code</a></h3>

<p>ScopeFlow revisits the following two parts in the conventional end-to-end training pipeline/protocol</p>
<ul>
  <li>Data augmentation:
    <ol>
      <li>photometric transformations: input image perturbation, such as color and gamma corrections.</li>
      <li>geometric augmentations: global or relative affine transformation, followed by random horizontal and vertical flipping.</li>
      <li>cropping</li>
    </ol>
  </li>
  <li>Regularization
    <ul>
      <li>weighted decay</li>
      <li>adding random Gaussian noises</li>
    </ul>
  </li>
</ul>

<p>and advocates</p>
<ul>
  <li>use larger scopes (crops and zoom-out) when possible.</li>
  <li>gradually reduce regularization</li>
</ul>

<h3 id="maskflownet-cvpr-2020-paper-code">MaskFlownet (CVPR 2020) <a href="https://arxiv.org/abs/2003.10955">paper</a> <a href="https://github.com/microsoft/MaskFlownet">code</a></h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/optical_flow/AsymOFMM-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/optical_flow/AsymOFMM-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/optical_flow/AsymOFMM-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/optical_flow/AsymOFMM.png" data-zoomable="" />

  </picture>

</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/optical_flow/MaskFlowNetS-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/optical_flow/MaskFlowNetS-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/optical_flow/MaskFlowNetS-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/optical_flow/MaskFlowNetS.png" data-zoomable="" />

  </picture>

</figure>

    </div>
</div>

<p>Key idea:</p>
<ul>
  <li>Incorporates a learnable occlusion mask that filters occluded areas immediately after feature warping without any explicit supervision.</li>
</ul>

<h3 id="raft-recurrent-all-pairs-field-transforms-for-optical-flow-arxiv-2020-paper-code">RAFT: REcurrent All-Pairs Field Transforms for Optical Flow (Arxiv 2020) <a href="https://arxiv.org/abs/2003.12039">paper</a> <a href="https://github.com/princeton-vl/RAFT">code</a></h3>]]></content><author><name></name></author><summary type="html"><![CDATA[summary of how optimal flow can be derived]]></summary></entry><entry><title type="html">Markov Chain Monte Carlo: Gibbs, Metropolis-Hasting, and Hamiltonian</title><link href="https://yyang768osu.github.io/blog/2019/markov-chain-monte-carlo/" rel="alternate" type="text/html" title="Markov Chain Monte Carlo: Gibbs, Metropolis-Hasting, and Hamiltonian" /><published>2019-07-06T00:00:00+00:00</published><updated>2019-07-06T00:00:00+00:00</updated><id>https://yyang768osu.github.io/blog/2019/markov-chain-monte-carlo</id><content type="html" xml:base="https://yyang768osu.github.io/blog/2019/markov-chain-monte-carlo/"><![CDATA[<p>A fundamental problem in statistical learning is to compute the expectation with respect to some target probability distribution \(\pi\)</p>

\[\begin{align*}
\mathbb{E}_\pi\left[f\right] \triangleq \int \pi(x) f(x) dx.
\end{align*}\]

<p>There are two difficulties in the evaluation of the above (1) often \(\pi(\cdot)\) is available to us only as a form of unnormalized probability, i.e., it can be evaluated only up to a normalizing constant (2) even if \(\pi(\cdot)\) can be evaluated exactly, it is often hard to directly generate samples from it (e.g., for high-dimensional space).</p>

<p>One example application is Bayesian inference, where the posterior probability of the latent \(\pi(x\|D)\) is available only in the form of prior \(\pi(x)\) times likelihood \(\pi(D\|x)\) up to the unknown normalizing constant of \(\pi(D)\), and we would like to either sample or obtain the expectation with respect to the posterior probability.</p>

<p>The idea of Markov Chain Monte Carlo (MCMC) is to construct a Markov chain whose stationary distribution is exactly the target distribution with easy-to-sample transition kernels. One could then start with a random initial state, and yield samples by simply running the transitions and use the generated samples after the chain reaches steady state for the Monte Carlo evaluation of the expectation.</p>

<p>For the design of such Markov chain, all methods that I encountered utilize the following theorem</p>
<blockquote>
  <p>An irreducible and aperiodic Markov chain with transition probability \(P\) has stationary distribution of \(\pi\) if it satisfies \begin{align} 
\pi(x)P(x’|x) = \pi(x’)P(x|x’) \notag
\end{align}</p>
</blockquote>

<p>The game, then, is to design \(P\) for which the above equality holds. In this article, we will go through three MCMC methods with different ways in the design of \(P\), namely <strong>Gibbs sampling</strong>, <strong>Metropolis-Hastings</strong>, and <strong>Hamiltonian Monte Carlo</strong> (HMC).</p>

<p>As a side note, it is worth pointing out that the above equation, referred to as <em>detailed balance equation</em>, is a sufficient but not necessary condition for a Markov chain to have stationary distribution \(\pi\). It defines a special case of Markov chain called reversible Markov chain. The detailed balance equation should be contrasted with <em>global balance equation</em> below, which all Markov chains with stationary distribution \(\pi\) satisfy. Then it shouldn’t be surprising that global balance equation can be easily derived from detailed balance equation (by summing over \(x'\) on both sides of the above equation) but not the other way around.</p>

\[\begin{align*}
\pi(x) = \sum_{x'} \pi(x')P(x'|x).
\end{align*}\]

<h2 id="gibbs-sampling">Gibbs sampling</h2>

<p>In Gibbs sampling, the transition probability \(P\) is defined as the following</p>

\[\begin{align*}
P\left(x'|x\right)=\left\{
\begin{array}{ll}
\frac{1}{d}\pi\left(x'_j|x_1, \ldots, x_{j-1}, x_{j+1}, \ldots, x_d\right) &amp; \text{if there exits }j\text{ such that }x_i'=x_i\text{ for }i\not=j.\\
0&amp;\text{otherwise.}
\end{array}
\right.
\end{align*}\]

<p>The state \(x\) is a vector of dimension \(d\), and the transition probability from state \(x\) to state \(x'\) is non-zero when they differ by only one dimension, say dimension \(j\), and the transition probability is designed to be the conditional probability of \(x'_j\), given all the other dimensions fixed, scaled by \(1/d\). This corresponds to a transition scheme where we uniformly pick a dimension \(j\), and then randomly sample a value in dimension \(j\) following the conditional distribution. Detailed balance equation holds with such design</p>

\[\begin{align*}
&amp;\pi(x)P(x'|x)\\
=&amp;\frac{1}{d}\pi(x)\pi(x'_j|x_1, \ldots, x_{j-1}, x_{j+1}, \ldots, x_d)\\
=&amp; \frac{1}{d}\pi(x)\pi(x')/\sum_{z}\pi(x_1, \ldots, x_{j-1}, z, x_{j+1}, \ldots, x_d)\\
=&amp;\frac{1}{d}\pi(x)\pi(x')/\sum_{z}\pi(x'_1, \ldots, x'_{j-1}, z, x'_{j+1}, \ldots, x'_d)\\
=&amp;\frac{1}{d}\pi(x')\pi(x_j|x'_1, \ldots, x'_{j-1}, x'_{j+1}, \ldots, x'_d)\\
=&amp;\pi(x')P(x|x').
\end{align*}\]

<p>The premise of Gibbs sampling is that the conditional distribution of one dimension given the rest is much easier to normalize and sample from. It is quite limited though, in the sense that the transition can never go very far in each step – only one dimension can be changed at a time. As a consequence, the transition matrix is quite sparse and the Markov chain may suffer from very large mixing time (time to stationary distribution) and it may not scale well with large dimensional space.</p>

<h2 id="metropolis-hastings">Metropolis Hastings</h2>

<p>Metropolis Hastings algorithm is a much more general version of Gibbs; in fact it encompasses both Gibbs sampling and Hamiltonian MC as special realizations. The basic idea is to construct the transition distribution from a flexible form of proposal distribution \(g(x'\|x)\), corrected by a <em>acceptance ratio</em> term \(A(x',x)\)  to guarantee reversibility in time. Specifically, the acceptance ratio is chosen to enforce the detailed balance equation</p>

\[\begin{align*}
\pi(x) g(x'|x) A(x', x) = \pi(x') g(x|x') A(x, x').
\end{align*}\]

<p>The actual transition probability is then \(P(x'\|x) \triangleq g(x'\|x) A(x', x)\), corresponding to a sampling scheme where we first sample from \(g(x'\|x)\) to have a candidate next state \(x'\), and then accept this candidate with probability \(A(x', x)\). If the candidate state is rejected, the next state will remain the same as the current state. For an arbitrary proposal distribution \(g\), from the above equation, we have</p>

\[\begin{align*}
\frac{A(x', x)}{A(x, x')} = \frac{\pi(x')g(x|x')}{\pi(x)g(x'|x)}.
\end{align*}\]

<p>To reduce the mixing time of the Markov chain, it is desirable to maximize the acceptance ratio \(A\). This means that we want to set either \(A(x',x)\) or \(A(x, x')\) to be \(1\) for any pair of \(x\) and \(x'\), resulting in the expression below</p>

\[\begin{align*}
A(x', x) = \min\left\{1, \frac{\pi(x')g(x|x')}{\pi(x)g(x'|x)}\right\}.
\end{align*}\]

<p>In the above equation, since \(\pi\) appear in both numerator and denominator, we can easily work with unnormalized probability distribution, as long as it can be evaluated efficiently for each data point.</p>

<p>Metropolis-Hasting algorithm itself is just a MCMC framework; it still relies on a good choice of proposal distribution to perform well. The design of \(g\) can be problem specific and is the <em>art</em>. The clear optimal choice of is \(g(x'\|x)=\pi(x)\), which degenerates to the direct sampling of \(\pi\) with acceptance ratio of \(1\).</p>

<h2 id="hamiltonian-monte-carlo">Hamiltonian Monte Carlo</h2>

<p>Let’s now image a high dimensional surface for which the potential energy at each point \(x\) is defined as \(V(x)\triangleq -\log\pi(x)\). Here we introduce an auxiliary variable \(p\) with the same dimension as \(x\), and interpret the pair of variable \((x, p)\) as describing the position and momentum of an object on the high dimensional space.</p>

<p>The kinetic energy of the object with mass \(m\) and momentum \(p\) is known as \(K(p)=\frac{p^2}{2m}\) (e.g., \(\frac{1}{2}mv^2 = (mv)^2/2m\)). We now construct a joint probability distribution of \((x,p)\) as</p>

\[\begin{align*}
\pi(x, p) = \frac{1}{Z}e^{-V(x)-K(p)} = \frac{1}{Z} e^{\log\pi(x)}e^{p^2/2m} = \frac{1}{Z}\pi(x)\mathcal{N}\left(p|0, \sqrt{m}\right).
\end{align*}\]

<p>Two remarks here: (1) The joint probability defined above is a function of the total energy \(V(x) + K(p)\) (potential energy plus kinetic energy) of the imaginary object. (2) Since the marginal distribution of \(\pi(x, p)\) with respect to \(x\) is \(\pi(x)\), if we can construct an effective MCMC algorithm for \((x, p)\), we then obtain an MCMC algorithm for \(x\) by discarding \(p\).</p>

<p>The key in Hamiltonian MC is to use Hamiltonian mechanism as a way to obtain new candidate state (corresponding to proposal \(g\) in Metropolis-Hastings).  Hamiltonian mechanics is an alternative reformation of the classic Newtonian mechanics describing Newton’s second law of motion. It characterizes the time evolution of the system in terms of location \(x\) and momentum \(p\), with the conservation of the sum of potential energy \(V(x)\) and Kinetic energy of \(K(p)\), a.k.a. Hamiltonian \(\mathcal{H}(x, p) \triangleq V(x) + K(p)\), through the following differential equations</p>

\[\begin{align*}
\frac{d p}{dt} =&amp; -\frac{\partial \mathcal{H}}{\partial x} &amp;\text{force equals to negative gradient of potential energy}\\
\frac{d x}{dt} =&amp; \frac{\partial \mathcal{H}}{\partial p} &amp;\text{velocity equals to derivative of kinetic energy w.r.t. momentum}
\end{align*}\]

<p>By solving the path of \((x, p)\) according to Hamiltonian mechanics, we are essentially traversing along the contour for which \(\pi (x, p)\) is fixed. This provide a very nice way of coming up with a proposal function \(g(x', p'\| x, p)\) without having to reject any candidate. In other words, if we start with the point \((x, p)\) and derive the system state \((x_\tau, p_\tau)\) after a period of time \(\tau\) , we then know that \(\pi(x, p) = \pi(x_\tau, p_\tau)\). If we further apply a negation in the momentum, then the proposal function is reversible.</p>

\[\begin{align*}
x, p \xrightarrow[]{\substack{\text{Hamiltonian}\\\text{mechanics}}} x_\tau, p_\tau \xrightarrow[]{\substack{\text{negate}\\\text{momentum}}} x_\tau, -p_\tau  = x', p'\\
x', p' \xrightarrow[]{\substack{\text{Hamiltonian}\\\text{mechanics}}} x'_\tau, p'_\tau \xrightarrow[]{\substack{\text{negate}\\\text{momentum}}}x'_\tau, -p'_\tau  = x, p
\end{align*}\]

<p>If we have perfect solver for the differential equation, then according to Equation (2) there is no need to reject any transition proposal. However, in reality the differential equation can only be solved in approximation with error, and thus \(\pi(x, p)\not=\pi(x', p')\), meaning that the acceptance ratio is not strictly \(1\) and certain fraction of the transition proposal would be rejected. It is worth noting that the method for computing the solution to the differential equation should still be reversible to respect the detailed balance equation. One hidden condition for such transition to be feasible is that the potential energy \(V(\cdot)\) has to be differentiable, implying that the target distribution \(\pi(\cdot)\) should be differentiable.</p>

<p>So now we have defined a proposal function according to Hamiltonian mechanics, which leads to large acceptance ratio. Are we done here? Not yet. If we stop here, then the Markov chain we defined is reducible, i.e., not every state is accessible from an initial state. In fact, we only have pairwise transition in the Markov chain. To ensure the sampling of the entire space, another proposal distribution \(g_2\) is introduced, taking advantage of the fact that \(\pi(x, p)\) has factorized form for which \(p\) follows a zero-mean normal distribution – the proposal distribution \(g_2\) simply samples the momentum value \(p\) from the corresponding marginal distribution. For such proposal, the corresponding acceptance ratio is \(1\)</p>

\[\begin{align*}
A((x, p'), (x, p)) = \min\left\{1, \frac{\pi(x, p')g_2(p|p')}{\pi(x, p)g_2(p'|p)}\right\} = \min\left\{1, \frac{\pi(x)}{\pi(x)}\right\}=1 .
\end{align*}\]

<p>Now we concatenate the above two proposals to have the final form of Hamiltonian MC sampling</p>

\[\begin{align*}
x, p_0 \xrightarrow[]{\substack{\text{resample}\\\text{momentum}}} x, p 
\xrightarrow[]{\substack{\text{Hamiltonian}\\\text{mechanics}}} x_\tau, p_\tau 
\xrightarrow[]{\substack{\text{negate}\\\text{momentum}}} x_\tau, -p_\tau  = x', p'.
\end{align*}\]

<p>Since every time after applying the Hamiltonian mechanics the momentum is resampled, we can ignore the momentum negation operation, leading to the following</p>

\[\begin{align*}
x \xrightarrow[]{\substack{\text{resample}\\\text{momentum}}} x, p 
\xrightarrow[]{\substack{\text{Hamiltonian}\\\text{mechanics}}} x_\tau, p_\tau 
\xrightarrow[]{\substack{\text{discard}\\\text{momentum}}} x_\tau = x',
\end{align*}\]

<p>and the corresponding acceptance ratio is</p>

\[\begin{align*}
A((x_\tau, p_\tau), (x, p)) = \min\left\{1, \frac{\pi(x_\tau, p_\tau)}{\pi(x, p)}\right\} =  \min\left\{1, e^{\mathcal{H}(x, p) - \mathcal{H}(x_\tau, p_\tau)}\right\}.
\end{align*}\]]]></content><author><name></name></author><summary type="html"><![CDATA[a primer on Gibbs sampling, Metropolis-Hasting, and Hamiltonian MC]]></summary></entry><entry><title type="html">Normalizing Flow: understanding the change of variable equation</title><link href="https://yyang768osu.github.io/blog/2019/normalizing-flow-change-of-variable-equation/" rel="alternate" type="text/html" title="Normalizing Flow: understanding the change of variable equation" /><published>2019-03-15T00:00:00+00:00</published><updated>2019-03-15T00:00:00+00:00</updated><id>https://yyang768osu.github.io/blog/2019/normalizing-flow-change-of-variable-equation</id><content type="html" xml:base="https://yyang768osu.github.io/blog/2019/normalizing-flow-change-of-variable-equation/"><![CDATA[<p>Normalizing flow is a technique for constructing complex probability distributions through invertible transformations of a simple distribution. It has been studied and applied in generative models under two contexts: (1) characterizing the approximation posterior distribution of latent variables in the case of variational inference (2) directly approximating the data distribution. When used in the second context, it has demonstrated its capability in generating high-fidelity audio, image, and video data.</p>

<p>The study of generative models is all about learning a distribution \(\mathbb{P}_{\mathcal{X}}\)  that fits the data \(\mathcal{X}\) well. With such distribution \(\mathbb{P}(\mathcal{X})\) we can, among other things, generate, by sampling from  \(\mathbb{P}_{\mathcal{X}}\), artificial data point that resembles \(\mathcal{X}\). Since the true data distribution lies in high-dimensional space and is potentially very complex, it is essential to have a parameterized distribution family that is flexible and expressive enough to approximate the true data distribution well.</p>

<p>The idea of flow-based methods is to <em>explicitly</em> construct a parameterized family of distributions by transforming a known distribution \(\mathbb{P}_{\mathcal{Z}}\), e.g., a standard multi-variant Gaussian, through a concatenation of function mappings. Let’s consider the elementary case of a single function mapping \(g\). For each sampled value \(z\) from \(\mathbb{P}_{\mathcal{Z}}\), we map it to a new value \(x=g(z)\).</p>

\[\begin{align*}
z \xrightarrow{g(.)}  x
\end{align*}\]

<p>Up until this point, we have not introduced anything new. This way of transforming a known distribution using a function mapping \(g\) is also adopted by generative adversarial networks (GAN). The question that flow-based method asks is: can we get a tractable probability density function (pdf) of \(x=g(z)\)? If so, we can <em>directly</em> optimize the probability density of the dataset, i.e., the log likelihood of the data, rather than resorting to the duality approach adopted by GAN, or the lower-bound approach adopted by VAE.</p>

<p>Unfortunately, for a general function \(g\) that maps \(z\) to \(x\), the pdf of the new random variable \(x=g(z)\) is quite complicated and usually intractable due to the need to calculate a multi-dimensional integral. However, if we restrict \(g\) to be a bijective (one-to-one correspondence) and differentiable function, then the general change-of-variable technique reduces to the following tractable form:</p>

\[\begin{align*}
\mathbb{P}_\mathcal{X}(x) =  \mathbb{P}_{\mathcal{Z}}(z)\left|\det \frac{d g(z)}{d z}\right|^{-1}, x=g(z)
\end{align*}\]

<p>An important consequence with the bijective assumption is that \(z\) and \(x\) must have the same dimension: if \(z\) is a \(d-\)dimensional vector \(z=[z_1, z_2, \ldots, z_d]\), the corresponds \(x\) must also be a \(d-\)dimensional vector \(x=[x_1, x_2, \ldots, x_d]\). It is worth emphasizing that the bijective assumption is essential to the tractability of the change-of-variable operation, and the resulting dimension invariance is a key restriction in flow-based methods.</p>

<p>The above equation, albeit tractable, looks by no means familiar or friendly — what is with the absolute value? the determinant? the Jacobian? the inverse? The whole equation screams for an intuitive explanation. So here we go — let’s gain some insights into the meaning of the formula.</p>

<p>First off, since \(g\) is bijective and thus invertible, we can denote the inverse of \(g\) as \(f=g^{-1}\), which allows us to rewrite the equation as</p>

\[\begin{align*}
\mathbb{P}_\mathcal{X}(x) =  \mathbb{P}_{\mathcal{Z}}(f(x))\left|\det \frac{d x}{d f(x)}\right|^{-1} =  \mathbb{P}_{\mathcal{Z}}(f(x))\left|\det \frac{d f(x)}{d x}\right|
\end{align*}\]

<p>In the last equation, we get ride of the inverse by resorting to the identity that the determinant of an inverse is the inverse of the determinant, the intuition of which will become clear later.</p>

<p>To understand the above equation, we start with a fundamental invariance in the change of probability random variables: <strong>the probability mass of the random variable \(z\) in any subset of \(\mathcal{Z}\) must be the same as the probability mass of \(x\) in the corresponding subset of \(\mathcal{X}\) induced by transformation from \(z\) to \(x\)</strong>, and vice versa.</p>

<p>Let us exemplify the above statement with an example. Consider the case when \(x\) and \(z\) are 2 dimensional, and focus on a small rectangular in \(\mathcal{X}\) defined by two corner points \((a, b)\) and \((a+\Delta x_1, b + \Delta x_2)\). If \(\Delta x_1\) and \(\Delta x_2\) are small enough, we can approximate the probability mass on the rectangular as the density \(\mathbb{P}_\mathcal{X}\) evaluated at point \((a,b)\) times the area of the rectangular. More precisely,</p>

\[\begin{align*}
&amp;P {\big(}  (x_1, x_2) \in [a, a+\Delta x_1]\times[b, b+\Delta x_2] {\big)}\\
\approx&amp; \mathbb{P}_\mathcal{X} ((a, b)) \times \text{area of }[a, a+\Delta x_1]\times[b, b+\Delta x_2]\\
=&amp;\mathbb{P}_\mathcal{X} ((a, b)) \Delta x_1 \Delta x_2
\end{align*}\]

<p>This approximation is basically assuming that the probabilistic density on the rectangular stays constant and equal to \(\mathbb{P}_\mathcal{X} ((a, b))\), which holds asymptotically true as we shrink the width \(\Delta x_1\) and height \(\Delta x_2\) of the rectangular. The left figure below provides an illustration.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/normalizing_flow/flow-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/normalizing_flow/flow-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/normalizing_flow/flow-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/normalizing_flow/flow.png" />

  </picture>

</figure>

    </div>
</div>

<p>Now resorting to the aforementioned invariance, the probability mass on the \(\Delta x_1 \times \Delta x_2\) rectangular must remain unchanged after the transformation. So what does the rectangular look like after the transformation of \(f\)? Let us focus on the corner point \((a+\Delta x_1, b)\):</p>

\[\begin{align*}
f((a+\Delta x_1, b))=&amp;(f_1(a+\Delta x_1,b), f_2(a+\Delta x_1,b))  \\
=&amp; (f_1(a,b), f_2(a,b))  \\
+&amp; \left(\frac{\partial f_1}{\partial x_1}(a, b)\Delta x_1 ,   \frac{\partial f_2}{\partial x_1}(a, b)\Delta x_1\right) \text{ }\text{ first order component} \\
+&amp; \left(o(\Delta x_1), o(\Delta x_1)\right) \text{ }\text{ second and higher order residual}
\end{align*}\]

<p>With \(\Delta x_1\) and \(\Delta x_2\) small enough, we can just ignore the higher order term and keep the linearized term. As can be seen from the figure above, the rectangular area is morphed into a parallelogram defined by the two vectors</p>

\[\begin{align*}
&amp;\left(\frac{\partial f_1}{\partial x_1}(a, b)\Delta x_1 ,   \frac{\partial f_2}{\partial x_1}(a, b)\Delta x_1\right)\\
&amp;\left(\frac{\partial f_1}{\partial x_2}(a, b)\Delta x_2 ,   \frac{\partial f_2}{\partial x_1}(a, b)\Delta x_2\right)
\end{align*}\]

<p>We have <a href="https://textbooks.math.gatech.edu/ila/determinants-volumes.html">geometry</a> to tell us that the area of a parallelogram is just the absolute determinant of the matrix composed of the edge vectors, which is expressed as below.</p>

\[\begin{align*}
{\Bigg|} \det \underbrace{\left[
\begin{array}{ll}
\frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_1}\\
\frac{\partial f_1}{\partial x_2} &amp; \frac{\partial f_2}{\partial x_2}
\end{array}
\right]_{(a,b)}}_{\substack{\text{Jacobian of $f$}\\\text{evaluated at $(a,b)$}}}
{\Bigg |} 
\Delta x_1 \Delta x_2
\end{align*}\]

<p>By plugging in the above into the invariance statement, we reached the following identity</p>

\[\begin{align*}
\mathbb{P}_\mathcal{X} ((a, b)) \Delta x_1 \Delta x_2 = \mathbb{P}_\mathcal{Z} (f(a, b)) \left|\det {\bf J}_f(a,b)\right| \Delta x_1 \Delta x_2
\end{align*}\]

<p>With \(\Delta x_1\Delta x_2\) canceled out, we reached our target equation</p>

\[\begin{align*}
\mathbb{P}_\mathcal{X} (x) = \mathbb{P}_\mathcal{Z} (f(x)) \left|\det {\bf J}_f(x)\right| = \mathbb{P}_\mathcal{Z} (f(x)) \left|\det \frac{\partial f(x)}{\partial x}\right|.
\end{align*}\]

<p>For data with dimension larger than two, the above equation still holds, with the distinctions that the parallelogram becomes a parallelepiped, and the concept of area becomes a more general notion of volume.</p>

<p>It should be clear now what the physical interpretation is for the absolute-determinant-of-Jacobian — it represents the <strong>local, linearized rate of volume change</strong> (quoted from <a href="https://blog.evjang.com/2018/01/nf1.html">this excellent blog</a>) for the function transform. Why do we care about the rate of volume change? exactly because of the invariance of probability measure — in order to make sure each volume holds the same measure of probability before and after the transformation, we need to factor in the volume change induced by the transformation.</p>

<p>With this interpretation that the absolute-determinant-of-Jacobian is just local linearized rate of volume change, it should not be surprising that the determinant of a Jacobian of an inverse function is the inverse of the determinant Jacobian of the original function. In other words, if function \(f\) expands a volume around \(x\) by rate of \(r\), then the inverse function \(g=f^{-1}\) must shrink a volume around \(f(x)\) by the same rate of \(r\).</p>]]></content><author><name></name></author><summary type="html"><![CDATA[decipher absolute-logarithm-determinant-Jocabian]]></summary></entry><entry><title type="html">Understanding conventional HMM-based ASR training</title><link href="https://yyang768osu.github.io/blog/2018/understanding-conventional-hmm-based-asr-training/" rel="alternate" type="text/html" title="Understanding conventional HMM-based ASR training" /><published>2018-11-17T00:00:00+00:00</published><updated>2018-11-17T00:00:00+00:00</updated><id>https://yyang768osu.github.io/blog/2018/understanding-conventional-hmm-based-asr-training</id><content type="html" xml:base="https://yyang768osu.github.io/blog/2018/understanding-conventional-hmm-based-asr-training/"><![CDATA[<p>Conventional HMM-based ASR system assumes a generative model comprised of a language model, a lexicon (e.g., pronunciation dictionary), and an acoustic model, as illustrated below. Here \(\theta\) denotes the parameters to be learned and it comprises of the HMM state transition probabilities and GMM/DNN parameters.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/asr/HMM-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/asr/HMM-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/asr/HMM-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/asr/HMM.png" />

  </picture>

</figure>

    </div>
</div>

<h2 id="maximum-likelihood-training">Maximum likelihood training</h2>

<p>In maximum likelihood estimation (MLE), as stated in the equation below, the objective is to maximize the likelihood of the data being generated by the generative model. In other words, we want to find the value of the parameters \(\theta\) so that the above model best explains the acoustic features (e.g., spectrogram) that we observe.</p>

\[\begin{align*}
&amp;\arg\max_\theta \prod_{n=1}^N \mathbb{P}\left({\bf x}^{(n)}|{\bf y}^{(n)};\theta\right)\\
=&amp;\arg\max_\theta \sum_{n=1}^N \log \mathbb{P}\left({\bf x}^{(n)}|{\bf y}^{(n)};\theta\right).
\end{align*}\]

<p>For ease of notation, for the rest of this section, let’s ignore the conditioning on \({\bf y}^{(n)}\) (or \({\bf p}^{(n)}\)). The difficulty in evaluating the above log-likelihood lies in the need to marginalize over all potential values of \({\bf z}^{(n)}\). This formulation falls right into the discussion of the previous two posts:  <a href="/blog/2018/variational-inference-I-variational-lower-bound/">variational lower bound</a> and <a href="/blog/2018/variational-inference-II-expectation-maximization/">expectation maximization</a>, which provide an iterative algorithm to approach the solution</p>

\[\begin{align}
\theta^{[i+1]} = \arg\max_{\theta} \sum_{n=1}^N \int \color{red}{\mathbb{P}\left({\bf z}^{(n)}| {\bf x}^{(n)};\theta^{[i]} \right)}\log \mathbb{P}\left({\bf x}^{(n)}, {\bf z}^{(n)};\theta\right)d z^{(n)}.
\end{align}\]

<p>Most of the computation complexity in the above equation lies in finding the posterior probability of the latent state given the observed \(\color{red}{\mathbb{P}\left({\bf z}^{(n)}\| {\bf x}^{(n)};\theta\right)}\). To elaborate on how the posterior probability is computed, let’s expand the acoustic model part in the previous figure as below, which is essentially a hidden-Markov chain.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/asr/HMM2-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/asr/HMM2-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/asr/HMM2-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/asr/HMM2.png" />

  </picture>

</figure>

    </div>
</div>

<p>The inference problem (finding the posterior of the latent given the observed) in a hidden Markov chain can be solved by a forward-backward algorithm. The algorithm manifests itself as BCJR algorithm in convolutional code bit-level MAP decoding and <a href="/blog/2018/kalman-filter-and-particle-filter/">Kalman filtering</a> in linear dynamic system.</p>

\[\begin{align}
\text{Forward path: }&amp;\text{calculate }\mathbb{P}\left(z_t,{\bf x}_{1\to t};\theta\right)\text{ from }\mathbb{P}\left(z_{t-1},{\bf x}_{1\to t-1};\theta\right) \notag \\
&amp;\mathbb{P}\left(z_t, {\bf x}_{1\to t};\theta\right) = \sum_{z_{t-1}} \mathbb{P}(z_{t}|z_{t-1};\theta)\mathbb{P}(x_{t}|z_{t};\theta)\mathbb{P}\left(z_{t-1},{\bf x}_{1\to t-1};\theta\right)  \notag \\
\text{Backward path: }&amp;\text{calculate }\mathbb{P}\left({\bf x}_{t+1\to T}{\big |}z_t;\theta\right)\text{ from }\mathbb{P}\left({\bf x}_{t+2\to T}{\big |}z_{t+1};\theta\right) \notag \\
&amp;\mathbb{P}\left({\bf x}_{t+1\to T}{\big |}z_t;\theta\right) = \sum_{z_{t+1}} \mathbb{P}(z_{t+1}|z_{t};\theta)\mathbb{P}(x_{t+1}|z_{t+1};\theta)\mathbb{P}\left({\bf x}_{t+2\to T}{\big |}z_{t+1};\theta\right) \notag \\
\text{Combined: }&amp;\mathbb{P}\left(z_t, {\bf x}_{1\to T};\theta\right) = \mathbb{P}\left(z_t,{\bf x}_{1\to t};\theta\right)\mathbb{P}\left({\bf x}_{t+1\to T}{\big |}z_t;\theta\right) \notag \\
&amp;\mathbb{P}\left(z_t| {\bf x}_{1\to T};\theta\right) = \mathbb{P}\left(z_t, {\bf x}_{1\to T};\theta\right) / \sum_{z_t}\mathbb{P}\left(z_t, {\bf x}_{1\to T};\theta\right)
\end{align}\]

<h2 id="circular-dependency-between-segmentation-and-recognition">Circular dependency between segmentation and recognition</h2>
<p>The expectation-maximization formulation for likelihood maximization reveals a fundamental circular dependency between segmentation and recognition.</p>

<p>Here <strong>segmentation</strong> refers to the alignment of sub-phoneme states of \({\bf y}\) and the acoustic feature observations \({\bf x}\), encoded in the hidden state sequence \({\bf z}\), and <strong>recognition</strong> refers to the classification of sub-phoneme hidden state sequence \({\bf z}\) for the corresponds acoustic feature observations \({\bf x}\).</p>

<p>The two equations below make the circular dependency precise:</p>

\[\begin{align*}
\theta^{[i]} &amp;
\underset{\text{ }}{\xrightarrow{\substack{\text{update soft-alignment}\\\text{based on recognition}}}} \mathbb{P}\left({\bf z}^{(n)}| {\bf x}^{(n)};\theta^{[i]} \right)\text{ using Equation (2)}\\
\mathbb{P}\left({\bf z}^{(n)}| {\bf x}^{(n)};\theta^{[i]} \right) &amp;
\underset{\text{ }}{\xrightarrow{\substack{\text{update recognition}\\\text{based on soft-alignment}}}}
\theta^{[i+1]}\text{ using Equation (1)}
\end{align*}\]

<p>It is easy to argue that to have an accurate alignment, we need accurate recognition, and to train an accurate recognition, we have to rely on accurate alignment/segmentation.</p>

<p>In a convention ASR system, to bootstrap the training procedure, we have to start with a dataset that has human curated phoneme boundary/segmentation. Once the system is capacitated with reasonable recognition/inference, it is no longer confined with human aligned dataset and a much larger dataset can be used with just waveform and the corresponding phoneme transcription. Eventually, after the system is able to deliver robust segmentation, we can make hard decision on the alignment, and only focus on improving the recognition performance with potentially a different system that has a much larger capacity, e.g., a DNN replacing the GMM model.</p>

<h2 id="decoding">Decoding</h2>
<p>In the decoding stage, we try to find the word/sentence with the maximum a posterior (MAP) probability given the observed data</p>

\[\begin{align*}
&amp;\arg\max_{\bf y} \mathbb{P}({\bf y}|{\bf x};\theta) \\
=&amp;\arg\max_{\bf y} \mathbb{P}({\bf x},{\bf y};\theta) / \mathbb{P}({\bf x};\theta)\\
\color{red}{=}&amp;\arg\max_{\bf y} \mathbb{P}({\bf x},{\bf y};\theta) \\
=&amp;\arg\max_{\bf y} \underbrace{\mathbb{P}({\bf x}|{\bf p};\theta)}_{\text{acoustic model}}\times\underbrace{\mathbb{P}({\bf p}|{\bf y})}_{\text{lexion}}\times\underbrace{\mathbb{P}({\bf y})}_{\text{language model}}
\end{align*}\]

<p>The lexicon and language model together construct a state transition diagram, which we unrolled in time to form a decoding trellis. For each transcription hypothesis, a proper MAP decoder would sum across all the paths in the trellis that corresponds to the transcription, which is computationally prohibitive.</p>

<p>One simplification one can make is to find the most probable path by running the Viterbi algorithm. However, even for Viterbi algorithm, the complexity is still too high for practical deployment due to the large state space and potentially large number of time steps.</p>

<p>To further reduce the computation complexity, the conventional system resorts to the beam-search algorithm – basically a breath-first-search algorithm on the trellis that maintain only a limited number of candidates. The beam-search algorithm is often run on a weighted finite state transducer that captures the concatenation of language model and lexicon.</p>

<h2 id="discrepancy-between-mle-training-and-map-decoding">Discrepancy between MLE training and MAP decoding</h2>

<p>At first glance into the MAP decoding equation, it may appear that the MLE based training is well-aligned with the decoding process: maximizing the posterior probability of the transcription \({\bf y}\) given the acoustic feature \({\bf x}\) is equivalent to maximizing the likelihood of the observation \({\bf x}\). The argument being that the probability of the observation \(\mathbb{P}(x;\theta)\) is anyway a constant dictated by the natural of people’s speech, not something we can control. But is it true?</p>

\[\begin{align*}
&amp;\text{inference time:}\\
&amp;\arg\max_{\bf y} \mathbb{P}({\bf y}|{\bf x};\theta) \\
=&amp;\arg\max_{\bf y} \mathbb{P}({\bf x},{\bf y};\theta) / \mathbb{P}({\bf x};\theta)\\
=&amp;\arg\max_{\bf y} \mathbb{P}({\bf x}|{\bf y};\theta)\mathbb{P}({\bf y})
\end{align*}\]

<p>It turns out there is a subtle difference between inference time (MAP decoding) and training time (MLE parameter update) that render the above statement wrong.</p>

\[\begin{align*}
&amp;\text{training time:}\\
&amp;\arg\max_{\color{red}{\theta}} \mathbb{P}({\bf y}|{\bf x};\theta) \\
=&amp;\arg\max_{\color{red}{\theta}} \mathbb{P}({\bf x},{\bf y};\theta) / \mathbb{P}({\bf x};\theta)\\
\color{red}{\not=}&amp;\arg\max_{\color{red}{\theta}} \mathbb{P}({\bf x}|{\bf y};\theta)\mathbb{P}({\bf y})
\end{align*}\]

<p>As is evident by comparing the above two equations, when we try to update parameter \(\theta\) to maximize directly the posterior probability of the transcription \({\bf y}\) given the acoustic feature \({\bf x}\), we can no longer ignore the term \(\mathbb{P}({\bf x};\theta)\). The key is to realize that we model the speech as a generative model, where <strong>the probability of observing a certain acoustic features \({\bf x}\) is not dictated by the nature, but rather the generative model that we assume</strong>. By updating the parameter \(\theta\) that best increase the likelihood, we inevitably change \(\mathbb{P}({\bf x};\theta)\) too, and thus there is no guarantee that the posterior probability is increased. \(\mathbb{P}({\bf x};\theta)\) is calculated by marginalizing over all potential transcriptions: \(\mathbb{P}({\bf x};\theta)=\sum_{\bf y}\mathbb{P}({\bf x}\|{\bf y};\theta)\).</p>

<p>To elaborate, in MLE, we try to maximize \(\color{red}{\mathbb{P}({\bf y}\|{\bf x};\theta)}\) with respect to \(\theta\), we may very well also increased the likelihood for competing transcription sequences \(\color{blue}{\mathbb{P}({\bf x}\|{\bf \tilde{y}};\theta)}\), potentially resulting in decreased posterior probability \(\mathbb{P}({\bf y}\|{\bf x};\theta)\).</p>

\[\begin{align*}
&amp;\mathbb{P}({\bf y}|{\bf x};\theta) \\
=&amp; \mathbb{P}({\bf x},{\bf y};\theta) / \mathbb{P}({\bf x};\theta)\\
=&amp; \frac{\color{red}{\mathbb{P}({\bf x}|{\bf y};\theta)}\mathbb{P}({\bf y})}{\sum_{\bf \tilde{y}}\color{blue}{\mathbb{P}({\bf x}|{\bf \tilde{y}};\theta)}\mathbb{P}({\bf \tilde{y}})}
\end{align*}\]

<p>Fundamentally, <strong>the misalignment is rooted from the fact that we are using a generative model for discriminative tasks</strong>. We end here by noting that several sequence discriminative training methods were proposed for better discrimination in the literature.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[call me an archeologist]]></summary></entry><entry><title type="html">Comparison of end-to-end ASR models</title><link href="https://yyang768osu.github.io/blog/2018/comparison-of-end-2-end-asr-models/" rel="alternate" type="text/html" title="Comparison of end-to-end ASR models" /><published>2018-11-13T00:00:00+00:00</published><updated>2018-11-13T00:00:00+00:00</updated><id>https://yyang768osu.github.io/blog/2018/comparison-of-end-2-end-asr-models</id><content type="html" xml:base="https://yyang768osu.github.io/blog/2018/comparison-of-end-2-end-asr-models/"><![CDATA[<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/asr/end2end-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/asr/end2end-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/asr/end2end-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/asr/end2end.png" />

  </picture>

</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/blog_img/asr/transcription_model-480.webp" />
    <source media="(max-width: 800px)" srcset="/assets/img/blog_img/asr/transcription_model-800.webp" />
    <source media="(max-width: 1400px)" srcset="/assets/img/blog_img/asr/transcription_model-1400.webp" />
    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/blog_img/asr/transcription_model.png" />

  </picture>

</figure>

    </div>
</div>]]></content><author><name></name></author><summary type="html"><![CDATA[how CTC, RNN-transducer, and Attention factor probabilities]]></summary></entry></feed>