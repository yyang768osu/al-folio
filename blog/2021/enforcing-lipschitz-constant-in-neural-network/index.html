<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>        
    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Yunfei  Huang | Enforcing Lipschitz Constant in Neural Network</title>
    <meta name="author" content="Yunfei  Huang" />
    <meta name="description" content="how to enforce lipschitz constraint in neural networks?" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

    <!-- Styles -->
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚓</text></svg>">
    <link rel="stylesheet" href="/Yunfei-Huang0.github.io/assets/css/main.css">
    <link rel="canonical" href="https://yunfei-huang0.github.io/Yunfei-Huang0.github.io/blog/2021/enforcing-lipschitz-constant-in-neural-network/">
  </head>

  <!-- Body -->
  <body class="fixed-top-nav">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://yunfei-huang0.github.io/Yunfei-Huang0.github.io//"><span class="font-weight-bold">Yunfei</span>   Huang</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/Yunfei-Huang0.github.io/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/Yunfei-Huang0.github.io/blog/">blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/Yunfei-Huang0.github.io/publications/">publications</a>
              </li>
            </ul>
          </div>
       </div>
      </nav>

    <!-- Scrolling Progress Bar -->
    <div class="progress-container fixed-top">
      <div class="progress-bar" id="myBar"></div>
    </div>

    <!-- Reading progress bar -->

    <!-- Javascript for Progress Bar -->
    <script type="text/javascript">
      window.onscroll = function() {myFunction()};

      function myFunction() {
        var winScroll = document.body.scrollTop || document.documentElement.scrollTop;
        var height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
        var scrolled = (winScroll / height) * 100;
        document.getElementById("myBar").style.width = scrolled + "%";
      }
    </script>
      
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Enforcing Lipschitz Constant in Neural Network</h1>
    <p class="post-meta">April 3, 2021</p>
    <p class="post-tags">
      <a href="/Yunfei-Huang0.github.io//blog/2021"> <i class="fas fa-calendar fa-sm"></i> 2021 </a>

    </p>
  </header>

  <article class="post-content">
    <p>A function \(g(x)\) is Lipschitz continuous if there exists a constant \(L\) such that \(\|g(x_1) - g(x_2)\| &lt; L \|x_1 - x_2\|\) for any \(x_1\) and \(x_2\) in its domain. \(L\) is referred to as a Lipschitz constant of \(g\). The need to enforce a certain Lipschitz constant of neural networks arises in many cases, with some examples listed below. Here we introduce a common technique used in many existing literatures.</p>

<ul>
  <li>Guarantee invertibility in normalizing flows build with residual blocks
    <ul>
      <li><a href="https://arxiv.org/abs/1811.00995" target="_blank" rel="noopener noreferrer">iResNet(ICML2019)</a></li>
    </ul>
  </li>
  <li>Discriminator regularization in GAN training
    <ul>
      <li><a href="https://arxiv.org/abs/1701.07875" target="_blank" rel="noopener noreferrer">Wasserstein-GAN(ICML2017)</a></li>
      <li><a href="https://arxiv.org/abs/1802.05957" target="_blank" rel="noopener noreferrer">SpectralNormalization(ICLR2018)</a></li>
    </ul>
  </li>
  <li>Improve network robustness against adversarial perturbations
    <ul>
      <li><a href="https://arxiv.org/abs/1802.04034" target="_blank" rel="noopener noreferrer">Lipschitz-margin-training(NIPS2018)</a></li>
    </ul>
  </li>
</ul>

<p>A small note before we proceed: Lipschitz continuous/constant is defined with respect to a choice of the norm \(\|\cdot\|\). Here we focus on 2-norm.</p>

<h2 id="lipschitz-constant-vs-spectral-norm-of-matrices">Lipschitz constant vs spectral norm of matrices</h2>

<p>Deep neural networks are typically build with interleaved linear layers (such as Conv, TConv, Pooling) and nonlinear activations (such as ReLU, sigmoid). The Lipschitz constant of most activation functions are either constant or easy to control, so we will only focus on linear operations. Linear operations in general can be expressed as in the form of matrix-vector product \(y = g(x) = Wx\) where \(W\) denotes a matrix. In this case, the smallest Lipschitz constant of \(g\) can be expressed as</p>

<p>\begin{equation}
\label{eq:lipconst}
\min_{x_1, x_2, x_1\not=x_2} \frac{
||g(x_1)-g(x_2)||
}{
||x_1 - x_2||
}
=
\min_{||v||\not=0}\frac{
||Wv||
}{
||v||
}
=
\min_{||v||=1}
||Wv||.
\end{equation}</p>

<p>The last term is also known as the <em>spectral norm</em> of matrix \(W\). Let us express \(W\) as its singular-value-decomposition \(U\Sigma V^T\), then we can see that the spectral norm of \(W\) is its maximum singular value, denoted as \(\sigma_1\). The maximum singular value of \(W\) is also the maximum eigenvalue of \(M\triangleq W^TW\) given that eigenvalues of \(W^TW\) is square of singular values of \(W\): \(M=V\Sigma U^TU\Sigma V^T=V\Sigma^2V^T=V\Lambda V^T\).</p>

<p>Now we know that obtaining the best Lipschitz constant of a linear operations amounts to finding the dominant singular value of its matrix representation \(W\), or dominant eigenvalue of \(M\triangleq W^TW\). Next let us introduce an iterative algorithm that can find it.</p>

<h2 id="power-method-aka-von-mises-iteration">Power method (aka Von Mises iteration)</h2>

<p>Power method finds the maximum eigenvalue of a matrix \(M\) using the following iteration:</p>

\[\begin{align*}
&amp;\text{start with a random vector }v^{(0)} \\
&amp;\text{for }k=1, 2, \ldots, \\
&amp;v^{(k)} = \frac{
M v^{(k-1)}
}{
||M v^{(k-1)}||
}
\end{align*}\]

<p>Claim: \(\|M v^{(k)}\|\) converges to the maximum eigen-value of \(M\) as \(k\) approaches infinity.</p>

<p>To show it, let us write the initial vector \(v^{(0)}\) as a linear combinations of eigen-vectors of \(M\): \(v^{(0)}=\sum_{i}\alpha_i v_i\), and expand the iterative formula as</p>

\[\begin{align*}
&amp;v^{(k)} = \frac{
M v^{(k-1)}
}{
||M v^{(k-1)}||
}
= \frac{
M^2 v^{(k-2)}
}{
||M^2v^{(k-2)}||
}=\ldots
= \frac{
M^k v^{(0)}
}{
||M^kv^{(0)}||
}
\\
&amp;M^k v^{(0)} = M^k \sum_{i} \alpha_i v_i = \sum_{i} \alpha_i M^k v_i =\sum_{i}\alpha_i \lambda_i^k v_i = \alpha_1\lambda_1^k
\left(
v_1 + \sum_{i&gt;1}\underbrace{\frac{\alpha_i}{\alpha_1}\left(\frac{\lambda_i}{\lambda_1}\right)^k}_{\to 0 \text{ as } k\to\infty} v_i
\right).
\end{align*}\]

<p>From the last equation we know that \(v^{(k)}\) converges to the dominant eigen-vector \(v_1\) of \(M\) up to a sign difference, and similarly \(Mv^{(k)}\) converges to the maximum eigen-value \(\sigma_1\) of \(M\).</p>

\[\begin{align*}
v^{(k)}\to\left\{\begin{array}{ll}
v_1 &amp; \text{if }\alpha_1&gt;0\\
-v_1 &amp; \text{if }\alpha_1&lt;0
\end{array}\right., \text{ as }k\to\infty
\end{align*}\]

<h2 id="compute-power-iteration-through-auto-differentiation">Compute power iteration through auto-differentiation</h2>

<p>From last section we know that the maximum singular value can be computed if we carry out the following iteration procedure:</p>

\[\begin{align*}
 v^{(k-1)} \Longrightarrow \underbrace{\tilde{v}^{(k)}=W^TWv^{(k-1)}}_{\text{step 1}} \Longrightarrow \underbrace{v^{(k)}=\tilde{v}^{(k)}/||\tilde{v}^{(k)}||}_{\text{step 2}} \Longrightarrow\ldots
\end{align*}\]

<p>While it is easy to compute vector norm as done in step 2, it is not immediately clear how to easily compute \(W^TWv^{(k-1)}\) in step 1, since expressing \(W\) explicitly for a general linear layer can be involved. For instance, for a 2D convolution operation, expressing it in the matrix-vector product form requires unpacking the convolution kernel into a doubly Toeplitz matrix. We know that \(Wv^{(k-1)}\) is just the output of the linear operator \(g\) when \(v^{(k-1)}\) is used as input, but seemingly there is no easy way to multiply by \(W^T\) without knowing \(W\) explicitly.</p>

<p>Here’s the trick: we can express \(W^TWx\) as the derivative of another another function and compute it with auto-differentiation.</p>

\[\begin{align*}
W^TW x = \frac{1}{2}\frac{\partial x^TW^TWx}{\partial x} = \frac{1}{2}\frac{\partial ||Wx||^2}{\partial x} =\frac{\partial \frac{1}{2}||g(x)||^2}{\partial x}
\end{align*}\]

<p>We can then modify the iteration procedure as</p>

\[\begin{align*}
 v^{(k-1)} \Longrightarrow \underbrace{
\tilde{v}^{(k)} = \frac{
\partial\frac{1}{2}||g(v^{v^{(k-1)}})||^2
}{
\partial v^{(k-1)}
}
 }_{\text{step 1}} \Longrightarrow \underbrace{v^{(k)}=\tilde{v}^{(k)}/||\tilde{v}^{(k)}||}_{\text{step 2}} \Longrightarrow\ldots
\end{align*}\]

<p>Based on last section, \(\sqrt{\|\tilde{v}^{(k)}\|}\) yields an estimate of the dominant singular value of \(M\), which is the Lipschitz constant of the linear operator \(g\).
In PyTorch, step 1 can be calculated using <a href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad" target="_blank" rel="noopener noreferrer">torch.atuograd.grad</a>.</p>

<p>It should not be surprising that the above iteration procedure converges to maximum singular value of \(W\) – it is simply the gradient ascent with Equation \eqref{eq:lipconst} as the optimization objective.</p>

<h2 id="enforce-lipschitz-constant-c-during-training">Enforce Lipschitz constant \(c\) during training</h2>

<p>It is easy to see that the Lipschitz constant of \(a\times g(\cdot)\) is \(a\) times the Lipschitz constant of \(g(\cdot)\), or more precisely, \(\text{Lip}(ag) = a\text{Lip}(g)\). To enforce the Lipschitz constant of an operator to be some target value \(c\), we just need to normalize the output the operator by \(c/\text{Lip}(g)\).</p>

<p>The power iteration procedure itself can be amortized and blended into the optimization step of the network training, in which case the training loop can be expressed as</p>

\[\begin{align*}
&amp;\text{for step }k=1, \ldots:\\
&amp;v = v/||v||\\
&amp;v = \frac{
\partial\frac{1}{2}||g(v)||^2
}{
\partial v 
}\\
&amp;\sigma = \sqrt{||v||}\\
&amp;\text{set the normalization scale of output of }g\text{ as }\frac{c}{\sigma}\\
&amp;\text{the rest of the training step}.
\end{align*}\]


  </article><div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname  = 'yyang768osu-github-io';
      var disqus_identifier = '/blog/2021/enforcing-lipschitz-constant-in-neural-network';
      var disqus_title      = "Enforcing Lipschitz Constant in Neural Network";
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" target="_blank" rel="noopener noreferrer">comments powered by Disqus.</a>
</noscript>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2022 Yunfei  Huang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.
Last updated: March 27, 2022.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Mansory & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/Yunfei-Huang0.github.io/assets/js/mansory.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/Yunfei-Huang0.github.io/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/Yunfei-Huang0.github.io/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123722738-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-123722738-1');
  </script>
  </body>
</html>

