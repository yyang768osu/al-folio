<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>        
    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Yang  Yang | A step-by-step guide to variational inference (2): expectation maximization</title>
    <meta name="author" content="Yang  Yang" />
    <meta name="description" content="how to optimize ELBO when your approx posterior can be easily obtained" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

    <!-- Styles -->
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚓</text></svg>">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://yyang768osu.github.io/blog/2018/variational-inference-II-expectation-maximization/">
  </head>

  <!-- Body -->
  <body class="fixed-top-nav">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://yyang768osu.github.io/"><span class="font-weight-bold">Yang</span>   Yang</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
            </ul>
          </div>
       </div>
      </nav>

    <!-- Scrolling Progress Bar -->
    <div class="progress-container fixed-top">
      <div class="progress-bar" id="myBar"></div>
    </div>

    <!-- Reading progress bar -->

    <!-- Javascript for Progress Bar -->
    <script type="text/javascript">
      window.onscroll = function() {myFunction()};

      function myFunction() {
        var winScroll = document.body.scrollTop || document.documentElement.scrollTop;
        var height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
        var scrolled = (winScroll / height) * 100;
        document.getElementById("myBar").style.width = scrolled + "%";
      }
    </script>
      
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">A step-by-step guide to variational inference (2): expectation maximization</h1>
    <p class="post-meta">July 15, 2018</p>
    <p class="post-tags">
      <a href="/blog/2018"> <i class="fas fa-calendar fa-sm"></i> 2018 </a>

    </p>
  </header>

  <article class="post-content">
    <p>In the previous post we went through the derivation of variational lower-bound, and showed how it helps convert the Bayesian inference and density estimation problem to an optimization problem. Let’s briefly recap the problem setup and restate some key points.</p>

<p>Consider a very general generative graphic model where each data point \(x^{(n)}\) is generated from a latent variable \(z^{(n)}\) conforming to a given distribution \(p(X\|Z;\theta)\), with \(z^{(n)}\) itself drawn from a given prior distribution \(p(Z; \theta)\). \(\theta\) captures the set of variables that the two probabilities are parameterized with. Two fundamental problems are to (1) estimate the density of existing dataset \(X\), i.e. \(p(X;\theta)\) and (2) derive the posterior probability of the latent variable \(Z\) given the observed data \(X\), i.e., \(p(Z\|X;\theta)\). The exact solution of both problems requires the evaluation of the often intractable integral \(\int P(X\| Z;\theta)P(Z;\theta)dZ\).</p>

<p>With the introduction of a variational/free distribution function \(q(Z)\), we have the following identity:</p>

\[\begin{align*}
\ln p(X;\theta) = \text{KL}{\big(}q||p(Z|X;\theta){\big)} + \mathcal{L}(q,\theta),
\end{align*}\]

<p>which says that the marginalized probability of dataset \(X\) can be decomposed into a sum of two terms with the first one being the KL divergence between \(q(Z)\) and the true posterior distribution \(p(Z\|X;\theta)\) and the second one expressed below.</p>

\[\begin{align*}
\mathcal{L}(q,\theta)=\int q(Z) \ln \frac{P(Z,X;\theta)}{q(Z)}dZ,
\end{align*}\]

<p>which is referred to as the variational lower bound: it is called a lower-bound as it is always less than \(\ln p(X;\theta)\) due to the non-negativity of KL divergence, and it is called variational as it is itself a functional that maps a variational/free distribution function \(q\) to a scalar value. This identity is quite exquisite in that it turns both the density estimation problem and the latent variable inference problem into an optimization problem, evident from the two equations below</p>

\[\begin{align*}
\ln p(X;\theta) &amp;= \max_{q}\mathcal{L}(q,\theta)\\
p(Z|X;\theta) &amp;= \arg\max_{q}\mathcal{L}(q,\theta).
\end{align*}\]

<p>The problem that Expectation Maximization algorithm designs to solve is the maximum-likelihood (ML) estimate of the parameter \(\theta\). Mind you that \(\theta\) is the parameter of the graphic model, and the task is to find a \(\theta\) such that the model best explain the existing data. In precise term, the problem is</p>

\[\begin{align*}
\max_{\theta}\ln p(X;\theta).
\end{align*}\]

<p>Now, resorting to the variational lower bound, equivalently we can also focus on the following maximization-maximization problem</p>

\[\begin{align*}
\max_{\theta}\ln p(X;\theta) = \max_{\theta}\max_{q}\mathcal{L}(q,\theta),
\end{align*}\]

<p>A natural question is: why would this be any easier to evaluate compared with maximizing \(\ln p(X;\theta)\) head on? did we increase our burden by considering a nested-maximization optimization problem rather than a single-maximization one?</p>

<p>To answer we need to have the objective function under scrutiny. Looking at the detailed expression of \(\mathcal{L}(q,\theta)\), the main hurdle is the evaluation the log-likelihood of the joint observed-latent variable \(p(Z,X;\theta)\). We want to emphasis that the two probability distributions \(p(Z;\theta)\) and \(p(X\|Z;\theta)\) are given as part of the model assumption, and they usually come in the form of well-known distributions, e.g., Gaussian, multinomial, exponential, etc. Thus, the joint likelihood of observed and hidden variable \(p(Z,X;\theta)=p(Z;\theta)p(X\|Z;\theta)\) is in an amenable form. Also, quite often, taking logarithm on it would break up all the multiplicative terms as summation, resulting in quite tractable from. Better yet, the parameters \(\theta\) that we need to compute gradient with, may naturally be decomposed into different terms in the summation, making the calculation of derivative easy with respect to individual parameters.</p>

<p>On the other hand, to compute the marginalized likelihood of the observed data only, i.e., \(P(X;\theta)\), one need to sum or integrate out the effect of \(Z\) from \(p(Z,X;\theta)\), which may lead to complicated expression. While the evaluation of \(P(X;\theta)\) may still be fine when, e.g., the marginalization only requires the summation of a finite number of terms (which is the case for the Gaussian mixture model), the real deal breaker is the difficulty in taking derivative of the log-likelihood with respective to the parameters: taking logarithm on \(P(X;\theta)\) almost surely won’t result in nice decomposition, as the logarithm is gated by the integral or summation, and the log-sum expression is a lot harder to break when we compute the derivative with respect to the parameters \(\theta\).</p>

<p>Coming back to the maximization-maximization problem, it is natural to devise an iterative algorithm that maximize the objective function \(\mathcal{L}(q,\theta)\) with alternating axis:</p>

\[\begin{align*}
\text{E step: }&amp;q^{(t)} = \arg\max_{q}\mathcal{L}(q,\theta^{(t)})\\
\text{M step: }&amp;\theta^{(t+1)} = \arg\max_{\theta}\mathcal{L}(q^{(t)}, \theta) 
\end{align*}\]

<p>It is worth mentioning that the first optimization problem is in general a very difficult one, as it requires searching through the whole function space. According to the derivation of the variational lower bound derivation we know that the optimal solution is the posterior distribution \(p(Z\|X;\theta^{(t)})\), which is hard to obtain. Actually finding an approximated posterior by maximizing the variational lower bound is the main theme in variational inference. Techniques of mean-field-approximation, and variational auto-encoder, which we cover in subsequent posts, targets at this problem.</p>

<p>To proceed, we make a very strong assumption that \(p(Z\|X;\theta^{(t)})\) can be easily obtained. As we will see later that with certain simple model (e.g., Gaussian mixture model), it is indeed a valid assumption, nevertheless it is the key assumption that significantly limits the application of the expectation maximization algorithm.</p>

<p>Anyway, for now let us live with this strong assumption, then the E-step results in the following  expression</p>

\[\begin{align*}
\text{E step: }q^{(t)} = p(Z|X;\theta^{(t)}).
\end{align*}\]

<p>Coming back to the second maximization problem (M-step), with \(q^{(t)}\) fixed, we can decompose the variational lower bound as</p>

\[\begin{align*}
\mathcal{L}(q^{(t)}, \theta) = \int q^{(t)}(Z)\ln p(X,Z;\theta)dZ + \int q^{(t)}(Z) \ln\frac{1}{q^{(t)}(Z)}dZ.
\end{align*}\]

<p>The second term above is just a constant term reflecting the entropy of \(q^{(t)}\), so let us ignore it, and then the second maximization problem reduces to</p>

\[\begin{align*}
\text{M step: }\theta^{(t+1)} =&amp;\max_{\theta} \int p(Z|X;\theta^{(t)}) \ln P(Z,X;\theta)dZ. 
\end{align*}\]

<p>The maximization target above can be viewed as finding the expectation of complete data (combining observed variable and latent variable) log likelihood, where the expectation is with respect to a fixed distribution on the latent variable \(Z\).</p>

<p>Let’s put the two steps together and review the whole iterative process. We are given a model with a set of parameters captured in \(\theta\). The task is find the values of the parameters \(\theta\) such that the model best explain the existing observed data at hand. At the beginning, we take a random guess on the value of the parameters. With that initial parameters, we find the posterior probability of the latent variable for each data point \(x\) in the training data set \(X\). Then, using that posterior probability, we calculate the expected complete-data log-likelihood, and try to find parameters \(\theta\) so that the complete-data log-likelihood is maximized. With \(\theta\) updated, we refresh our calculation of the posterior probability and iterative the process.</p>

<p>In fact, K-means clustering algorithm is one instance of expectation-maximization procedure with certain model assumption. It is helpful to think of the E-M iterative process from the perspective of K-means clustering: for K-means clustering, the latent variable is one-dimensional with value from \(1\) to \(K\), implying the registration to one of the \(K\) clusters. The parameter of the model is the center of the clusters, denoted as \(\theta=\{c_1, \ldots, c_K\}\). In the initial setup, we randomly set these \(K\) cluster centers. For each data, we assign it to the nearest cluster, which is effectively assigning its latent variable. This step corresponds to finding the posterior distribution (E-step), with one of the clustering having probability \(1\). After each data is assigned to its cluster with the initial values of the cluster centers, which gives us complete data in the form of (observed data, latent variable) pair, the next step is to adjust the center based on its constituent. This step corresponds to the maximizing of the expected complete-data log-likelihood (M-step), although this expectation is taken in a degenerated form as the posterior probability for the latent variable is in the form of \(0/1\).</p>

<p>We finish the treatment of E-M algorithm with the following closing remarks:</p>

<ol>
  <li>The E-M iterative algorithm is guaranteed to reach a local maximum on the log-likelihood of the observed data \(p(X;\theta)\), as both steps increases it.</li>
  <li>It is not necessary to find the maximum in the M-step. So long as the updated \(\theta\) increase the complete-data log-likelihood, we are still in the right direction.</li>
  <li>So far we focused on finding the maximum-likelihood (ML) solution to \(\theta\) (local maximum). In the case when there is prior distribution \(p_\text{prior}(\theta)\) on \(\theta\), we can use the same process to find a maximum-a-posterior (MAP) solution (local maximum), utilizing the fact that \(p(\theta\|X) \propto p(X\|\theta)p_\text{prior}(\theta)\). The problem is modified as</li>
</ol>

\[\begin{align*}
\max_{\theta}\ln p(\theta|X) = \max_{\theta}\left(\max_{q}\mathcal{L}(q,\theta) {\color{red} + \ln p_\text{prior}(\theta)}\right),
\end{align*}\]

<p>with slightly modified procedure below</p>

\[\begin{align*}
\text{E step: }&amp;q^{(t)} = p(Z|X,\theta^{(t)})\\
\text{M step: }&amp;\theta^{(t+1)} =\max_{\theta} \int p(Z|X,\theta^{(t)}) \ln P(Z,X|\theta)dZ {\color{red} + \ln p_\text{prior}(\theta)}. 
\end{align*}\]


  </article><div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname  = 'yyang768osu-github-io';
      var disqus_identifier = '/blog/2018/variational-inference-II-expectation-maximization';
      var disqus_title      = "A step-by-step guide to variational inference (2): expectation maximization";
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" target="_blank" rel="noopener noreferrer">comments powered by Disqus.</a>
</noscript>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2022 Yang  Yang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.
Last updated: May 25, 2022.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Mansory & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/mansory.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123722738-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-123722738-1');
  </script>
  </body>
</html>

